"""Redis Stream helper for Lumina event bus.

This module provides a standardized interface for publishing and consuming events
via Redis Streams, with proper typing, retry logic, and error handling.
"""

import json
import logging
import asyncio
from typing import Any, Dict, Optional, Callable, Awaitable, List, Tuple
import redis.asyncio as aioredis
from redis.exceptions import ResponseError
from pydantic import BaseModel, Field
from datetime import datetime

logger = logging.getLogger(__name__)

class StreamMessage(BaseModel):
    """Base model for stream messages with metadata."""
    id: str = Field(..., description="Message ID (auto-generated by Redis)")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    data: Dict[str, Any] = Field(..., description="Message payload")
    stream: str = Field(..., description="Source stream name")
    group: Optional[str] = Field(None, description="Consumer group name")

class BusClient:
    """Redis Stream client with retry logic and proper error handling."""
    
    def __init__(
        self,
        redis_url: str = "redis://redis:6379",
        max_retries: int = 3,
        retry_delay: float = 1.0,
        encoding: str = "utf-8"
    ):
        """Initialize the bus client.
        
        Args:
            redis_url: Redis connection URL
            max_retries: Maximum number of retries for operations
            retry_delay: Delay between retries in seconds
            encoding: Redis encoding
        """
        self.redis_url = redis_url
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.encoding = encoding
        self._redis: Optional[aioredis.Redis] = None
        
    async def connect(self) -> None:
        """Establish Redis connection."""
        if self._redis is None:
            self._redis = aioredis.from_url(
                self.redis_url,
                encoding=self.encoding,
                decode_responses=True
            )
            try:
                await self._redis.ping()
            except Exception as e:
                logger.error(f"Failed to connect to Redis: {e}")
                raise
                
    async def close(self) -> None:
        """Close Redis connection."""
        if self._redis is not None:
            await self._redis.close()
            self._redis = None
            
    async def _with_retry(self, operation: Callable[..., Awaitable[Any]], *args, **kwargs) -> Any:
        """Execute operation with retry logic."""
        last_error = None
        for attempt in range(self.max_retries):
            try:
                if self._redis is None:
                    await self.connect()
                return await operation(*args, **kwargs)
            except (aioredis.ConnectionError, aioredis.TimeoutError) as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay * (attempt + 1))
                    self._redis = None  # Force reconnect
                continue
            except Exception as e:
                logger.error(f"Unexpected error in bus operation: {e}")
                raise
        raise last_error or Exception("Max retries exceeded")
        
    async def publish(
        self,
        stream: str,
        data: Dict[str, Any],
        maxlen: Optional[int] = None
    ) -> str:
        """Publish message to stream.
        
        Args:
            stream: Stream name
            data: Message payload
            maxlen: Optional maximum stream length (trims oldest entries)
            
        Returns:
            Message ID
        """
        async def _publish():
            # Add timestamp to payload
            payload = {
                "timestamp": datetime.utcnow().isoformat(),
                **data
            }
            # Convert all values to JSON strings
            stream_data = {k: json.dumps(v) for k, v in payload.items()}
            args = [stream, stream_data]
            if maxlen is not None:
                args.extend(["MAXLEN", "~", str(maxlen)])
            return await self._redis.xadd(*args)
            
        return await self._with_retry(_publish)
        
    async def consume(
        self,
        stream: str,
        group: str,
        consumer: str,
        handler: Callable[[StreamMessage], Awaitable[None]],
        block_ms: int = 5000,
        count: int = 10,
        start_id: str = ">"
    ) -> None:
        """Consume messages from stream with consumer group.
        
        Args:
            stream: Stream name
            group: Consumer group name
            consumer: Consumer name
            handler: Async handler function
            block_ms: Block time in milliseconds
            count: Maximum messages to fetch per batch
            start_id: Starting message ID (">" for new messages)
        """
        async def _ensure_group():
            try:
                await self._redis.xgroup_create(stream, group, id="0", mkstream=True)
            except ResponseError as e:
                if "BUSYGROUP" not in str(e):
                    raise
                logger.debug(f"Consumer group {group} already exists")
                
        async def _consume():
            while True:
                try:
                    # Ensure group exists
                    await _ensure_group()
                    
                    # Read messages
                    response = await self._redis.xreadgroup(
                        group, consumer,
                        {stream: start_id},
                        count=count,
                        block=block_ms
                    )
                    
                    if not response:
                        continue
                        
                    for stream_name, messages in response:
                        for msg_id, data in messages:
                            # Parse message
                            try:
                                # Convert JSON strings back to Python objects
                                parsed_data = {
                                    k: json.loads(v) for k, v in data.items()
                                }
                                message = StreamMessage(
                                    id=msg_id,
                                    stream=stream_name,
                                    group=group,
                                    data=parsed_data
                                )
                                
                                # Process message
                                await handler(message)
                                
                                # Acknowledge
                                await self._redis.xack(stream_name, group, msg_id)
                                
                            except json.JSONDecodeError as e:
                                logger.error(f"Failed to parse message {msg_id}: {e}")
                                # Still ack to prevent blocking
                                await self._redis.xack(stream_name, group, msg_id)
                            except Exception as e:
                                logger.error(f"Error processing message {msg_id}: {e}")
                                # Don't ack on handler error
                                
                except Exception as e:
                    logger.error(f"Error in consume loop: {e}")
                    await asyncio.sleep(1)  # Prevent tight loop on errors
                    
        await self._with_retry(_consume)
        
    async def get_stream_info(self, stream: str) -> Dict[str, Any]:
        """Get stream information including length and consumer groups.
        
        Args:
            stream: Stream name
            
        Returns:
            Stream information dictionary
        """
        async def _get_info():
            info = await self._redis.xinfo_stream(stream)
            groups = await self._redis.xinfo_groups(stream)
            return {
                "length": info["length"],
                "groups": groups,
                "last_generated_id": info["last-generated-id"],
                "first_entry": info.get("first-entry"),
                "last_entry": info.get("last-entry")
            }
        return await self._with_retry(_get_info)
        
    async def get_consumer_lag(self, stream: str, group: str) -> int:
        """Get consumer group lag (pending messages).
        
        Args:
            stream: Stream name
            group: Consumer group name
            
        Returns:
            Number of pending messages
        """
        async def _get_lag():
            info = await self._redis.xinfo_groups(stream)
            for g in info:
                if g["name"] == group:
                    return g["pending"]
            return 0
        return await self._with_retry(_get_lag) 