# Docker Compose configuration for Lumina system (V2)
# --------------------------------------------

version: '3.8'

name: lumina-system

networks:
  backend:
    name: lumina-system_backend
    driver: bridge

services:
  redis:
    image: redis:7-alpine
    command: redis-server --requirepass 02211998
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - backend

  ingest-gateway:
    build:
      context: .
      dockerfile: services/ingest_gateway/Dockerfile
    ports:
      - "8601:8601"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - QUEUE_STREAM=ingest.queue
      - FP_SET=ingest.fp
      - MIN_LICENSE_CONFIDENCE=0.8
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8601/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  dead-letter-ui:
    build:
      context: .
      dockerfile: services/dead-letter-ui/Dockerfile
    ports:
      - "8602:8602"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - DEAD_LETTER_API_KEY=${LUMINA_API_KEY:-changeme}
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8602/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  # Busybox that keeps Streams alive (creates groups on first boot)
  redis-streams-init:
    image: busybox:1.36
    command: sh -c "sleep infinity"
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend

  redis-exporter:
    image: oliver006/redis_exporter:latest
    command: --redis.addr=redis://:02211998@redis:6379
    ports:
      - "9121:9121"
    depends_on:
      - redis

  neo4j:
    image: neo4j:5.13.0
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms_memory_heap_initial__size=1G
      - NEO4J_dbms_memory_heap_max__size=1G
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_plugins:/plugins
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - backend

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - backend
    healthcheck:
      test: ["CMD", "test", "1" ]
      interval: 10s
      timeout: 5s
      retries: 3

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - backend
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS=nomic-embed-text
      - OLLAMA_ORIGINS=*
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/version && curl -f -X POST http://localhost:11434/api/generate -H 'Content-Type: application/json' -d '{\"model\": \"nomic-embed-text\", \"prompt\": \"test\"}' || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 180s
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./services/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - "9090:9090"
    depends_on:
      - redis-exporter

  grafana:
    image: grafana/grafana:10.0.3
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus

  promtail:
    image: grafana/promtail:latest
    volumes:
      - ./logs:/var/log/app
      - ./promtail-config.yml:/etc/promtail/promtail-config.yml
    command: -config.file=/etc/promtail/promtail-config.yml
    networks:
      - backend
    restart: unless-stopped

  loki:
    image: grafana/loki:2.8.2
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - backend
    restart: unless-stopped

  jaeger:
    image: jaegertracing/all-in-one:1.48
    ports:
      - "16686:16686"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "14268:14268"
      - "14250:14250"
      - "9411:9411"
    networks:
      - backend
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    networks:
      - backend
    restart: unless-stopped

  auditor:
    build:
      context: ./services/auditor
      dockerfile: Dockerfile
    ports:
      - "8811:8811"
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8811/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      - redis

  debugger:
    build:
      context: ./services/debugger
      dockerfile: Dockerfile
    ports:
      - "8814:8814"
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8814/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      - redis

  drift-exporter:
    build:
      context: ./services/drift-exporter
      dockerfile: Dockerfile
    ports:
      - "8816:8816"
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8816/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      - redis

  monitoring:
    build:
      context: ./services/monitoring
      dockerfile: Dockerfile
    ports:
      - "8824:8824"
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8824/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      - redis

  crawler:
    build:
      context: .
      dockerfile: services/crawler/Dockerfile
    ports:
      - "8400:8400"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - QDRANT_URL=http://qdrant:6333
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=nomic-embed-text
      - TRAINING_DATA_PATH=/app/training_data
      - CHUNK_SIZE=500
      - CHUNK_OVERLAP=100
      - BATCH_SIZE=8
      - PROCESS_INTERVAL=3600
      - INQUIRY_CHECK_INTERVAL=300
      - RETRAIN_INTERVAL=100
      - RATE_LIMIT_REQUESTS=10
      - RATE_LIMIT_PERIOD=60
    volumes:
      - ./training_data:/app/training_data
    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8400/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  event-mux:
    build:
      context: ./services/event-mux
      dockerfile: Dockerfile
    ports:
      - "8817:8817"
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8817/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      - redis

  batch-embedder:
    build:
      context: .
      dockerfile: services/batch-embedder/Dockerfile
    ports:
      - "8709:8709"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - EMBED_MODEL=nomic-embed-text
      - TRAIN_URL=http://concept-trainer-growable:8710/train_batch
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8709/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  output-engine:
    build:
      context: .
      dockerfile: services/output-engine-v2/Dockerfile
    ports:
      - "9000:9000"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - CONCEPT_DICT_URL=http://concept-dictionary:8828
      - TRAINER_URL=http://concept-trainer-growable:8710
      - OUTPUT_ENGINE_API_KEY=changeme
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - HEALTH_CHECK_TIMEOUT=30
      - MAX_RETRIES=3
      - RETRY_DELAY=5
      - STARTUP_TIMEOUT=120
    volumes:
      - adapters_data:/adapters
    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
      concept-dictionary:
        condition: service_healthy
      concept-trainer-growable:
        condition: service_healthy
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/health && curl -f http://localhost:9000/ready || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  concept-dictionary:
    build:
      context: ./services/concept-dictionary
      dockerfile: Dockerfile
    ports:
      - "8828:8828"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - QDRANT_URL=http://qdrant:6333
      - CONCEPT_DICT_API_KEY=${LUMINA_API_KEY:-changeme}
    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8828/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  graph-api:
    build:
      context: .
      dockerfile: services/graph-api/Dockerfile
    ports:
      - "8200:8200"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=password
      - QDRANT_URL=http://qdrant:6333
      - SERVICE_PORT=8200
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8200/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  concept-trainer:
    image: alpine
    command: tail -f /dev/null
    ports:
      - "8831:8831"
    networks:
      - backend
    restart: unless-stopped

  concept-trainer-growable:
    build:
      context: ./services/concept-trainer-growable
      dockerfile: Dockerfile
    volumes:
      - trainer_batches:/data/batches
      - trainer_adapters:/adapters
      - trainer_state:/app/data
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - QDRANT_URL=http://qdrant:6333
      - DEVICE=cuda
      - DICT_URL=http://concept-dictionary:8828
      - REDIS_PASSWORD=02211998
    ports:
      - "8710:8710"  # API
      - "8711:8711"  # Metrics
    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      concept-dictionary:
        condition: service_healthy
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8710/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  concept-trainer-embedder:
    image: alpine
    command: tail -f /dev/null
    ports:
      - "8833:8833"
    networks:
      - backend
    restart: unless-stopped

  concept-trainer-embedder-growable:
    image: alpine
    command: tail -f /dev/null
    ports:
      - "8834:8834"
    networks:
      - backend
    restart: unless-stopped

  concept-analytics:
    build:
      context: ./services/concept-analytics
      dockerfile: Dockerfile
    ports:
      - "8835:8835"
    networks:
      - backend
    restart: unless-stopped

  concept-analyzer:
    build:
      context: ./services/concept-analyzer
      dockerfile: Dockerfile
    ports:
      - "8836:8836"
    networks:
      - backend
    restart: unless-stopped

  learning-graph:
    build:
      context: ./services/learning-graph
      dockerfile: Dockerfile
    ports:
      - "8837:8837"
    networks:
      - backend
    restart: unless-stopped

  dual-chat-router:
    build:
      context: .
      dockerfile: services/dual-chat-router/Dockerfile
    ports:
      - "8300:8300"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - OUTPUT_ENGINE_URL=http://output-engine:9000
      - OLLAMA_URL=http://ollama:11434
      - OUTPUT_ENGINE_API_KEY=changeme
    depends_on:
      redis:
        condition: service_healthy
      output-engine:
        condition: service_healthy
      ollama:
        condition: service_started
    networks:
      - backend
    restart: unless-stopped

  masterchat-core:
    build:
      context: ./services/masterchat-core
      dockerfile: Dockerfile
    ports:
      - "8839:8839"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
    volumes:
      - adapters_data:/adapters
    networks:
      - backend
    restart: unless-stopped

  feedback-logger:
    build:
      context: ./services/feedback_logger
      dockerfile: Dockerfile
    ports:
      - "8900:8900"
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - FEEDBACK_LOGGER_API_KEY=${LUMINA_API_KEY:-changeme}
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8900/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    restart: unless-stopped

# --- Disabled: No directory or Dockerfile found for masterchat-llm ---
# masterchat-llm:
#   build:
#     context: ./services/masterchat-llm
#     dockerfile: Dockerfile
#   ports:
#     - "8840:8840"
#   networks:
#     - backend
#   restart: unless-stopped

# --- UI service intentionally disabled for now ---
# ui:
#   build:
#     context: ./services/ui
#     dockerfile: Dockerfile
#   networks:
#     - backend
#   ports:
#     - "80:80"
#   restart: unless-stopped

  retrain-listener:
    build:
      context: ./retrain-listener
      dockerfile: Dockerfile
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - TRAIN_URL=http://batch-embedder:8709/train_batch
      - SOURCE_STREAM=output.generated
      - BATCH_SIZE=64
    ports:
      - "8680:8680"
    depends_on:
      redis:
        condition: service_healthy
      batch-embedder:
        condition: service_healthy
    networks:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8680/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  graph-concept-adapter:
    build:
      context: ./services/graph-concept-adapter
      dockerfile: Dockerfile
    ports:
      - "8842:8842"
    networks:
      - backend
    restart: unless-stopped

# --- Disabled: No directory or Dockerfile found for data-analyzer ---
# data-analyzer:
#   build:
#     context: ./services/data-analyzer
#     dockerfile: Dockerfile
#   ports:
#     - "8843:8843"
#   networks:
#     - backend
#   restart: unless-stopped

# --- Disabled: No directory or Dockerfile found for log-embedding-worker ---
# log-embedding-worker:
#   build:
#     context: ./services/log-embedding-worker
#     dockerfile: Dockerfile
#   ports:
#     - "8846:8846"
#   networks:
#     - backend
#   restart: unless-stopped

# --- Disabled: No directory or Dockerfile found for learning-path-optimizer ---
# learning-path-optimizer:
#   build:
#     context: ./services/learning-path-optimizer
#     dockerfile: Dockerfile
#   ports:
#     - "8844:8844"
#   networks:
#     - backend
#   restart: unless-stopped

# --- Disabled: No directory or Dockerfile found for trend-analyzer ---
# trend-analyzer:
#   build:
#     context: ./services/trend-analyzer
#     dockerfile: Dockerfile
#   ports:
#     - "8845:8845"
#   networks:
#     - backend
#   restart: unless-stopped

  rag-coordinator:
    build:
      context: ./services/rag-coordinator
      dockerfile: Dockerfile
    environment:
      - REDIS_URL=redis://:02211998@redis:6379
      - CONCEPT_DICT_URL=http://concept-dictionary:8828
    depends_on:
      concept-dictionary:
        condition: service_healthy
    networks:
      - backend
    restart: unless-stopped

volumes:
  redis_data:
    driver: local
  neo4j_data:
  neo4j_logs:
  neo4j_plugins:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  qdrant_data:
    driver: local
  ollama_data:
  output-engine-temp:
  adapters_data:
    driver: local
  trainer_batches:
  trainer_adapters:
  trainer_state:
  crawler_data:
    driver: local

# Remaining services (not yet added) in the stack:
# --------------------------------------------
# (Core Services)
# – concept-dict
# – graph-api
# – concept-trainer (and its growable variants)
# – concept-trainer-embedder (and its growable variants, including gpu variants)
# – concept-trainer-embedder-growable-gpu-{1,…, 230} (if applicable)
# – concept-analytics
# – concept-analyzer
# – learning-graph
# (Chat & LLM)
# – dual-chat-router
# – masterchat-core
# (UI)
# – ui 
# – ui 