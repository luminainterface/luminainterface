# LLM Configuration
LLM_PROVIDER=mistral  # Options: mistral, openai, anthropic, etc.
LLM_MODEL=mistral-medium
MAX_TOKENS=1024
TEMPERATURE=0.7

# System Configuration
LLM_CACHE_DIR=data/llm_cache
ENABLE_LLM_CACHING=true

# Mistral API Configuration
MISTRAL_API_KEY=2AyKmqCkChQ75bseJTLK9QF2AK0aefJPs   

# Add other API keys and configuration as needed 