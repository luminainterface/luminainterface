# üåü ULTIMATE AI ORCHESTRATION ARCHITECTURE v10 - COMPLETE CONTAINERIZATION
# Revolutionary 3-Tier Strategic Steering System with 34+ Services
# 
# ARCHITECTURE LAYERS:
# 1. üß† HIGH-RANK ADAPTER - Ultimate Strategic Steering
# 2. üéØ META-ORCHESTRATION CONTROLLER - Strategic Logic  
# 3. ‚ö° ENHANCED EXECUTION SUITE - 8-Phase Orchestration
# 4. üßÆ V5 ENHANCED MATHEMATICAL ORCHESTRATOR - SYMPY VERIFICATION
#
# One Command: docker compose -f docker-compose-v10-ultimate.yml up -d

services:
  # ============================================================================
  # üß† LAYER 1: HIGH-RANK ADAPTER - ULTIMATE STRATEGIC STEERING
  # ============================================================================
  
  # üåü High-Rank Adapter - Ultimate Strategic Layer
  high-rank-adapter:
    build:
      context: .
      dockerfile: ./services/high-rank-adapter/Dockerfile
    container_name: high-rank-adapter
    ports:
      - "9000:9000"  # Ultimate Strategic Steering Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
      - ENHANCED_EXECUTION_HOST=enhanced-execution-suite
      - ENHANCED_EXECUTION_PORT=8998
      # Strategic Steering Parameters
      - TRANSCRIPT_INFLUENCE=0.8
      - PATTERN_SENSITIVITY=0.7
      - EVOLUTION_AGGRESSIVENESS=0.6
      - SELF_REFLECTION_DEPTH=0.9
      - QUALITY_PRIORITIZATION=0.85
    depends_on:
      redis:
        condition: service_healthy
      meta-orchestration-controller:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./high_rank_adapter.py:/app/high_rank_adapter.py
      - ./meta_orchestration_controller.py:/app/meta_orchestration_controller.py
      - ./conversation_transcripts:/app/transcripts
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 3G
        limits:
          cpus: '6.0'
          memory: 8G

  # ============================================================================
  # üéØ LAYER 2: META-ORCHESTRATION CONTROLLER - STRATEGIC LOGIC
  # ============================================================================
  
  # üéØ Meta-Orchestration Controller - Strategic Decision Making
  meta-orchestration-controller:
    build:
      context: .
      dockerfile: ./services/meta-orchestration/Dockerfile
    container_name: meta-orchestration-controller
    ports:
      - "8999:8999"  # Strategic Logic Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
      - MULTI_CONCEPT_DETECTOR_PORT=8860
      # Strategic Orchestration Parameters
      - CONCEPT_DETECTION_IMPORTANCE=0.8
      - VERIFICATION_THOROUGHNESS=0.7
      - SPEED_VS_QUALITY_BALANCE=0.6
      - RESEARCH_DEPTH_PREFERENCE=0.5
    depends_on:
      redis:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
      rag-coordination-interface:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./meta_orchestration_controller.py:/app/meta_orchestration_controller.py
      - ./enhanced_real_world_benchmark.py:/app/enhanced_real_world_benchmark.py
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8999/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.5'
          memory: 2G
        limits:
          cpus: '4.0'
          memory: 6G

  # ============================================================================
  # ‚ö° LAYER 3: ENHANCED EXECUTION SUITE - 8-PHASE ORCHESTRATION
  # ============================================================================
  
  # ‚ö° Enhanced Execution Suite - 8-Phase Orchestrated Generation
  enhanced-execution-suite:
    build:
      context: .
      dockerfile: ./services/enhanced-execution/Dockerfile
    container_name: enhanced-execution-suite
    ports:
      - "8998:8998"  # Enhanced Execution Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
      - MULTI_CONCEPT_DETECTOR_PORT=8860
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - SWARM_INTELLIGENCE_HOST=swarm-intelligence-engine
      - SWARM_INTELLIGENCE_PORT=8977
      - OLLAMA_HOST=godlike-ollama
      - OLLAMA_PORT=11434
      # Enhanced Execution Parameters
      - ENABLE_CONCEPT_DETECTION=true
      - ENABLE_WEB_SEARCH=true
      - ENABLE_VERIFICATION_MODULES=true
      - MAX_ORCHESTRATION_PHASES=8
    depends_on:
      redis:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
      rag-coordination-interface:
        condition: service_healthy
      multi-concept-detector:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./enhanced_real_world_benchmark.py:/app/enhanced_real_world_benchmark.py
      - ./unsat_guard.py:/app/unsat_guard.py
      - ./constraint_mask.py:/app/constraint_mask.py
      - ./token_limiter.py:/app/token_limiter.py
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8998/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 4G
        limits:
          cpus: '6.0'
          memory: 12G

  # ============================================================================
  # üéÑüåü CENTRAL UNIFIED THINKING ENGINE - THE BRAIN
  # ============================================================================
  
  neural-thought-engine:
    build:
      context: ./services/neural-thought-engine
      dockerfile: Dockerfile
    container_name: neural-thought-engine
    ports:
      - "8890:8890"  # Central Brain Port from flow2.md
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - NEO4J_HOST=neo4j
      - NEO4J_PORT=7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=thinking123
      - OLLAMA_HOST=godlike-ollama
      - OLLAMA_PORT=11434
      # Maximum Steering Coordination
      - PHI2_HOST=phi2-ultrafast-engine
      - PHI2_PORT=8892
      - RAG_HOST=rag-coordination-interface
      - RAG_PORT=8952
      # üåü GOLD STAR FEATURES (ONLY ONE STAR!)
      - BIDIRECTIONAL_THINKING=true
      - CONSCIOUSNESS_SIMULATION=true
      - A2A_AGENTS_ENABLED=true
      - EIGHT_PHASE_REASONING=true
      - TOOL_COORDINATION=true
      - PERFORMANCE_TARGET=51.5
      - DIMINISHING_RETURNS_DETECTION=true
      - CIRCUIT_BREAKERS=true
      # Ultimate Architecture Integration
      - HIGH_RANK_ADAPTER_HOST=high-rank-adapter
      - HIGH_RANK_ADAPTER_PORT=9000
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      godlike-ollama:
        condition: service_healthy
      phi2-ultrafast-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8890/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 2G
        limits:
          cpus: '4.0'
          memory: 4G

  # ============================================================================
  # üß† NEURAL COORDINATION & A2A COMMUNICATION
  # ============================================================================
  
  # ü§ù A2A Coordination Hub - Agent-to-Agent Communication
  a2a-coordination-hub:
    build:
      context: ./services/a2a-coordination-hub
      dockerfile: Dockerfile
    container_name: a2a-coordination-hub
    ports:
      - "8891:8891"
    environment:
      - THINKING_ENGINE_HOST=neural-thought-engine
      - THINKING_ENGINE_PORT=8890
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      neural-thought-engine:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8891/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üåå Collaborative Quantum Agent - Quantum A2A Coordination
  collaborative-quantum-agent:
    build:
      context: ./services/collaborative-quantum-agent
      dockerfile: Dockerfile
    container_name: collaborative-quantum-agent
    ports:
      - "8975:8975"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - A2A_HUB_HOST=a2a-coordination-hub
      - A2A_HUB_PORT=8891
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - SWARM_INTELLIGENCE_HOST=swarm-intelligence-engine
      - SWARM_INTELLIGENCE_PORT=8977
      # Quantum Processing Parameters
      - ENABLE_QUANTUM_SUPERPOSITION=true
      - ENABLE_QUANTUM_ENTANGLEMENT=true
      - ENABLE_QUANTUM_INTERFERENCE=true
      - ENABLE_WAVE_FUNCTION_COLLAPSE=true
      - QUANTUM_OBSERVATION_TYPE=weighted_random
      - QUANTUM_COHERENCE_THRESHOLD=0.7
      - ENTANGLEMENT_STRENGTH_THRESHOLD=0.5
    depends_on:
      redis:
        condition: service_healthy
      a2a-coordination-hub:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./services/collaborative-quantum-agent/collaborative_quantum_agent.py:/app/collaborative_quantum_agent.py
      - ./quantum_processing_logs:/app/logs
    labels:
      - "service.type=quantum-coordination"
      - "service.capabilities=superposition,entanglement,interference,wave-function-collapse"
      - "service.quantum_phases=5"
      - "service.integration=a2a-neural-coordination"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8975/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 2G
        limits:
          cpus: '3.0'
          memory: 4G

  # üêù Swarm Intelligence Engine - Collective Intelligence
  swarm-intelligence-engine:
    build:
      context: ./services/swarm-intelligence-engine
      dockerfile: Dockerfile
    container_name: swarm-intelligence-engine
    ports:
      - "8977:8977"
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - A2A_HUB_HOST=a2a-coordination-hub
      - A2A_HUB_PORT=8891
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      neural-thought-engine:
        condition: service_healthy
      a2a-coordination-hub:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8977/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ü§ñ Neural Memory Bridge - Advanced Memory Management
  neural-memory-bridge:
    build:
      context: ./services/neural-memory-bridge
      dockerfile: Dockerfile
    container_name: neural-memory-bridge
    ports:
      - "8892:8892"
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
    depends_on:
      neural-thought-engine:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8892/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # üìö RAG¬≤ ENHANCED KNOWLEDGE ORCHESTRATION
  # ============================================================================
  
  # üéØ RAG Coordination Interface - WITH CONCEPT DETECTION
  rag-coordination-interface:
    build:
      context: ./services/rag-coordination-interface
      dockerfile: Dockerfile
    container_name: rag-coordination-interface
    ports:
      - "8952:8952"  # Enhanced with concept detection
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_ORCHESTRATOR_PORT=8953
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
      - MULTI_CONCEPT_DETECTOR_PORT=8860
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Enhanced Concept Detection Features
      - ENABLE_CONCEPT_ROUTING=true
      - ENABLE_PERFORMANCE_METRICS=true
      - ENABLE_CROSS_SERVICE_VALIDATION=true
    depends_on:
      neural-thought-engine:
        condition: service_healthy
      multi-concept-detector:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8952/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üîÄ RAG Router Enhanced - Smart query distribution
  rag-router-enhanced:
    build:
      context: ./services/rag-router-enhanced
      dockerfile: Dockerfile
    container_name: rag-router-enhanced
    ports:
      - "8951:8951"
    environment:
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      rag-coordination-interface:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8951/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üìã RAG Orchestrator - Central RAG coordination
  rag-orchestrator:
    build:
      context: ./services/rag-orchestrator
      dockerfile: Dockerfile
    container_name: rag-orchestrator
    ports:
      - "8953:8953"
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - VECTOR_STORE_HOST=vector-store
      - VECTOR_STORE_PORT=9262
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      neural-thought-engine:
        condition: service_healthy
      vector-store:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8953/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üî• RAG GPU Long - Complex analysis processing
  rag-gpu-long:
    build:
      context: ./services/rag-gpu-long
      dockerfile: Dockerfile
    container_name: rag-gpu-long
    ports:
      - "8920:8920"
    environment:
      - RAG_ORCHESTRATOR_HOST=rag-orchestrator
      - RAG_ORCHESTRATOR_PORT=8953
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      rag-orchestrator:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8920/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üï∏Ô∏è RAG Graph - Graph-based knowledge retrieval
  rag-graph:
    build:
      context: ./services/rag-graph
      dockerfile: Dockerfile
    container_name: rag-graph
    ports:
      - "8921:8921"
    environment:
      - NEO4J_HOST=neo4j
      - NEO4J_PORT=7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=thinking123
      - RAG_ORCHESTRATOR_HOST=rag-orchestrator
      - RAG_ORCHESTRATOR_PORT=8953
    depends_on:
      neo4j:
        condition: service_healthy
      rag-orchestrator:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8921/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üíª RAG Code - Code-specific knowledge processing
  rag-code:
    build:
      context: ./services/rag-code
      dockerfile: Dockerfile
    container_name: rag-code
    ports:
      - "8922:8922"
    environment:
      - RAG_ORCHESTRATOR_HOST=rag-orchestrator
      - RAG_ORCHESTRATOR_PORT=8953
      - VECTOR_STORE_HOST=vector-store
      - VECTOR_STORE_PORT=9262
    depends_on:
      rag-orchestrator:
        condition: service_healthy
      vector-store:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8922/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ‚ö° RAG CPU Optimized - Fast processing
  rag-cpu-optimized:
    build:
      context: ./services/rag-2025-cpu-optimized
      dockerfile: Dockerfile
    container_name: rag-cpu-optimized
    ports:
      - "8902:8902"
    environment:
      - RAG_ORCHESTRATOR_HOST=rag-orchestrator
      - RAG_ORCHESTRATOR_PORT=8953
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      rag-orchestrator:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8902/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # ‚ö° LORA¬≤ ENHANCED GENERATION
  # ============================================================================
  
  # üéØ LoRA Coordination Hub - Central LoRA orchestration
  lora-coordination-hub:
    build:
      context: ./services/lora-coordination-hub
      dockerfile: Dockerfile
    container_name: lora-coordination-hub
    ports:
      - "8995:8995"
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      neural-thought-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8995/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ‚ö° Enhanced Prompt LoRA - Advanced prompt enhancement
  enhanced-prompt-lora:
    build:
      context: ./services/enhanced-prompt-lora
      dockerfile: Dockerfile
    container_name: enhanced-prompt-lora
    ports:
      - "8880:8880"
    environment:
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
    depends_on:
      lora-coordination-hub:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üöÄ Optimal LoRA Router - Smart LoRA routing
  optimal-lora-router:
    build:
      context: ./services/optimal-lora-router
      dockerfile: Dockerfile
    container_name: optimal-lora-router
    ports:
      - "5030:5030"
    environment:
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
    depends_on:
      lora-coordination-hub:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5030/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üé≠ Quality Adapter Manager - Quality control
  quality-adapter-manager:
    build:
      context: ./services/quality-adapter-manager
      dockerfile: Dockerfile
    container_name: quality-adapter-manager
    ports:
      - "8992:8992"  # Changed port to avoid conflict with LoRA Coordination Hub (8995)
    environment:
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      lora-coordination-hub:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8992/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # üîß COORDINATED TOOLS & CONCEPT DETECTION
  # ============================================================================
  
  # üéØ Multi-Concept Detector - Enhanced concept detection
  multi-concept-detector:
    build:
      context: ./services/multi-concept-detector
      dockerfile: Dockerfile
    container_name: multi-concept-detector
    ports:
      - "8860:8860"
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - COORDINATION_MODE=true
    depends_on:
      neural-thought-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8860/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üß† Concept Training Worker - Advanced concept learning
  concept-training-worker:
    build:
      context: ./services/concept-training-worker
      dockerfile: Dockerfile
    container_name: concept-training-worker
    ports:
      - "8851:8851"
    environment:
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
      - MULTI_CONCEPT_DETECTOR_PORT=8860
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      multi-concept-detector:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8851/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üîç Enhanced Crawler NLP - Advanced web crawling
  enhanced-crawler-nlp:
    build:
      context: ./services/enhanced-crawler-nlp
      dockerfile: Dockerfile
    container_name: enhanced-crawler-nlp
    ports:
      - "8850:8850"
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      neural-thought-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8850/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # üîç ENHANCED FACT-CHECKING LAYER V4 - PARTIAL PRODUCTION DEPLOYMENT
  # ============================================================================
  
  # üîç Enhanced Fact Checker V4 - PARTIAL PRODUCTION (Technology, Medicine, Psychology, Geography)
  enhanced-fact-checker:
    build:
      context: ./services/enhanced-fact-checker
      dockerfile: Dockerfile.v4
    container_name: enhanced-fact-checker-v4-partial
    ports:
      - "8885:8885"  # Enhanced Fact-Checking Port
    environment:
      - ENHANCED_PROMPT_LORA_HOST=enhanced-prompt-lora
      - ENHANCED_PROMPT_LORA_PORT=8880
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
      - MULTI_CONCEPT_DETECTOR_PORT=8860
      - RAG_CPU_OPTIMIZED_HOST=rag-cpu-optimized
      - RAG_CPU_OPTIMIZED_PORT=8902
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      - OLLAMA_HOST=godlike-ollama  # Updated to use container name
      - OLLAMA_PORT=11434
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # V4 Partial Production Configuration
      - V4_PRODUCTION_MODE=true
      - V4_PARTIAL_DOMAINS=TECHNOLOGY,MEDICINE,PSYCHOLOGY,GEOGRAPHY
      - V4_DEPLOYMENT_STATUS=PARTIAL_PRODUCTION
      - V5_DEVELOPMENT_DOMAINS=CHEMISTRY,ENGINEERING,SPORTS,GLOBAL_ISSUES,EDGE_CASES
      # V4 Fact-Checking Parameters
      - ENABLE_DATABASE_VERIFICATION=true
      - ENABLE_DOMAIN_FILTERING=true
      - PRODUCTION_DOMAINS_ONLY=false  # Allow processing of non-production domains with warnings
      - CLAIM_EXTRACTION_THRESHOLD=0.7
      - CORRECTION_CONFIDENCE_THRESHOLD=0.8
      - V4_PROCESSING_TIME_TARGET=0.05  # 50ms target
      - V4_ERROR_DETECTION_TARGET=0.80  # 80% error detection
    depends_on:
      enhanced-prompt-lora:
        condition: service_healthy
      multi-concept-detector:
        condition: service_healthy
      rag-cpu-optimized:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./fact_checking_v4_production.db:/app/fact_checking_v4_production.db
      - ./enhanced_fact_checking_layer_v4_production.py:/app/enhanced_fact_checking_layer_v4_production.py
    labels:
      - "service.type=fact-checking"
      - "service.version=v4-partial"
      - "service.coverage=partial"
      - "service.production_domains=TECHNOLOGY,MEDICINE,PSYCHOLOGY,GEOGRAPHY" 
      - "service.development_domains=CHEMISTRY,ENGINEERING,SPORTS,GLOBAL_ISSUES,EDGE_CASES"
      - "service.deployment_status=APPROVED_PARTIAL_PRODUCTION"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8885/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 2G
        limits:
          cpus: '3.0'
          memory: 4G

  # ============================================================================
  # üìÑ RESEARCH PAPER GENERATION SUITE - PUBLICATION EXCELLENCE
  # ============================================================================
  
  # üèÜ Enhanced Research Backend - Publication Excellence Generator
  enhanced-research-backend:
    build:
      context: .
      dockerfile: ./services/research-paper-generation/Dockerfile
    container_name: enhanced-research-backend
    ports:
      - "5000:5000"  # Research Paper Generation API
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - OLLAMA_HOST=godlike-ollama
      - OLLAMA_PORT=11434
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      - ENHANCED_FACT_CHECKER_HOST=enhanced-fact-checker
      - ENHANCED_FACT_CHECKER_PORT=8885
      - AI_DETECTION_MITIGATOR_HOST=ai-detection-mitigator
      - AI_DETECTION_MITIGATOR_PORT=5001
      # Research Paper Generation Parameters
      - ENABLE_AI_FAILSAFE_TRAPS=true
      - ENABLE_INFINITE_ELABORATION=true
      - ENABLE_AI_HUMANIZATION=true
      - ENABLE_ENHANCED_FACT_CHECKING=true
      - TARGET_QUALITY_SCORE=9.5
      - MAX_HUMANIZATION_LEVEL=10
      - INFINITE_ELABORATION_MAX_DEPTH=50
      # Integration with Ultimate Architecture
      - HIGH_RANK_ADAPTER_HOST=high-rank-adapter
      - HIGH_RANK_ADAPTER_PORT=9000
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
      - ENHANCED_EXECUTION_HOST=enhanced-execution-suite
      - ENHANCED_EXECUTION_PORT=8998
      - V5_MATH_ORCHESTRATOR_HOST=v5-enhanced-math-orchestrator
      - V5_MATH_ORCHESTRATOR_PORT=8990
    depends_on:
      redis:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
      rag-coordination-interface:
        condition: service_healthy
      enhanced-fact-checker:
        condition: service_healthy
      godlike-ollama:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./enhanced_research_backend.py:/app/enhanced_research_backend.py
      - ./ai_failsafe_trap_system.py:/app/ai_failsafe_trap_system.py
      - ./publication_excellence_demo.py:/app/publication_excellence_demo.py
      - ./publication_excellence_v2.py:/app/publication_excellence_v2.py
      - ./research_paper_frontend.html:/app/templates/research_paper_frontend.html
      - ./research_papers_generated:/app/output
    labels:
      - "service.type=research-paper-generation"
      - "service.capabilities=publication-excellence,infinite-elaboration,ai-humanization"
      - "service.integration=ultimate-architecture"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.5'
          memory: 3G
        limits:
          cpus: '4.0'
          memory: 8G

  # ü§ñ‚û°Ô∏èüë§ AI Detection Mitigator - Human-like Output Generation
  ai-detection-mitigator:
    build:
      context: .
      dockerfile: ./services/ai-detection-mitigation/Dockerfile
    container_name: ai-detection-mitigator
    ports:
      - "5001:5001"  # AI Detection Mitigation API
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      # AI Detection Mitigation Parameters
      - DETECTION_ANALYSIS_ENABLED=true
      - HUMANIZATION_INTENSITY_MAX=1.0
      - PATTERN_DETECTION_SENSITIVITY=0.7
      - ENABLE_REAL_TIME_ANALYSIS=true
      - RISK_THRESHOLD_HIGH=0.7
      - RISK_THRESHOLD_MEDIUM=0.4
      # Humanization Strategies
      - ENABLE_STRUCTURAL_IMPERFECTIONS=true
      - ENABLE_SPECIFICITY_REDUCTION=true
      - ENABLE_UNCERTAINTY_MARKERS=true
      - ENABLE_TONE_VARIATIONS=true
      - ENABLE_NATURAL_PACING=true
      - ENABLE_PRECISION_REDUCTION=true
    depends_on:
      redis:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./ai_detection_mitigation.py:/app/ai_detection_mitigation.py
      - ./humanization_patterns:/app/patterns
      - ./detection_analysis_logs:/app/logs
    labels:
      - "service.type=ai-humanization"
      - "service.capabilities=detection-analysis,pattern-mitigation,humanization"
      - "service.patterns=perfect-structure,effortless-specifics,unwavering-confidence,perfect-consistency,compressed-density,fictional-precision"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 2G
        limits:
          cpus: '3.0'
          memory: 4G

  # üìù Research Paper Frontend Interface - Publication Excellence UI
  research-paper-frontend:
    build:
      context: .
      dockerfile: ./services/research-paper-frontend/Dockerfile
    container_name: research-paper-frontend
    ports:
      - "3005:3000"  # Research Paper Frontend Port
    environment:
      - RESEARCH_BACKEND_HOST=enhanced-research-backend
      - RESEARCH_BACKEND_PORT=5000
      - AI_DETECTION_HOST=ai-detection-mitigator
      - AI_DETECTION_PORT=5001
      - ENABLE_REAL_TIME_PREVIEW=true
      - ENABLE_AUTO_SAVE=true
      - ENABLE_INFINITE_RECURSION_UI=true
      - SAMPLE_DATA_PRELOAD=true
    depends_on:
      enhanced-research-backend:
        condition: service_healthy
      ai-detection-mitigator:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./research_paper_frontend.html:/app/public/index.html
      - ./frontend_assets:/app/public/assets
    labels:
      - "service.type=research-paper-frontend"
      - "service.features=chatgpt-style-interface,infinite-recursion,real-time-editing,sample-data-preload"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üåä Infinite Elaboration Engine - Unlimited Research Depth
  infinite-elaboration-engine:
    build:
      context: .
      dockerfile: ./services/infinite-elaboration/Dockerfile
    container_name: infinite-elaboration-engine
    ports:
      - "5002:5002"  # Infinite Elaboration API
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      - RESEARCH_BACKEND_HOST=enhanced-research-backend
      - RESEARCH_BACKEND_PORT=5000
      # Infinite Elaboration Parameters
      - MAX_ELABORATION_DEPTH=infinity
      - ELABORATION_FOCUS_AREAS=literature_review,methodology,data_analysis,results,ethics,future_research
      - ENABLE_REAL_TIME_PROGRESS=true
      - QUALITY_ENHANCEMENT_PER_ITERATION=0.1
      - WORD_COUNT_EXPANSION_TARGET=300
      - ENABLE_CIRCUIT_BREAKERS=true
      - MAX_ITERATIONS_PER_SESSION=100
    depends_on:
      enhanced-research-backend:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
      rag-coordination-interface:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./infinite_elaboration_machine.py:/app/infinite_elaboration_machine.py
      - ./elaboration_sessions:/app/sessions
      - ./elaboration_logs:/app/logs
    labels:
      - "service.type=infinite-elaboration"
      - "service.capabilities=unlimited-depth,real-time-progress,quality-enhancement"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.5'
          memory: 3G
        limits:
          cpus: '4.0'
          memory: 6G

  # üß™ Research System Demo Engine - Comprehensive Demonstration
  research-system-demo:
    build:
      context: .
      dockerfile: ./services/research-demo/Dockerfile
    container_name: research-system-demo
    ports:
      - "5003:5003"  # Demo System Port
    environment:
      - RESEARCH_BACKEND_HOST=enhanced-research-backend
      - RESEARCH_BACKEND_PORT=5000
      - AI_DETECTION_HOST=ai-detection-mitigator
      - AI_DETECTION_PORT=5001
      - INFINITE_ELABORATION_HOST=infinite-elaboration-engine
      - INFINITE_ELABORATION_PORT=5002
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Demo Configuration
      - ENABLE_AI_DETECTION_DEMO=true
      - ENABLE_HUMANIZATION_DEMO=true
      - ENABLE_INFINITE_ELABORATION_DEMO=true
      - ENABLE_INTEGRATION_DEMO=true
      - DEMO_SAMPLE_PAPERS=true
    depends_on:
      enhanced-research-backend:
        condition: service_healthy
      ai-detection-mitigator:
        condition: service_healthy
      infinite-elaboration-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./demo_research_system.py:/app/demo_research_system.py
      - ./demo_outputs:/app/demo_outputs
    labels:
      - "service.type=research-demo"
      - "service.purpose=system-demonstration,integration-testing"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # üèóÔ∏è INFRASTRUCTURE SERVICES
  # ============================================================================
  
  # üî¥ Redis - Primary caching and coordination
  redis:
    image: redis:7-alpine
    container_name: godlike-redis
    ports:
      - "6379:6379"
    command: redis-server --requirepass 02211998 --appendonly yes
    networks:
      - ultimate-network
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # üìä Qdrant - Vector database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: godlike-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    networks:
      - ultimate-network
    volumes:
      - qdrant-data:/qdrant/storage
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # üï∏Ô∏è Neo4j - Graph database
  neo4j:
    image: neo4j:5.15-community
    container_name: godlike-neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/thinking123
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    networks:
      - ultimate-network
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:7474"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # ü¶ô Ollama - LLM serving
  godlike-ollama:
    image: ollama/ollama:latest
    container_name: godlike-ollama
    ports:
      - "11434:11434"
    networks:
      - ultimate-network
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # üóÇÔ∏è Vector Store - Enhanced vector storage
  vector-store:
    build:
      context: ./services/vector-store
      dockerfile: Dockerfile
    container_name: vector-store
    ports:
      - "9262:9262"
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9262/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üìù Transcript Ingest - Conversation logging
  transcript-ingest:
    build:
      context: ./services/transcript-ingest
      dockerfile: Dockerfile
    container_name: transcript-ingest
    ports:
      - "9264:9264"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - VECTOR_STORE_HOST=vector-store
      - VECTOR_STORE_PORT=9262
    depends_on:
      redis:
        condition: service_healthy
      vector-store:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9264/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # üî¨ ADVANCED PROCESSING SERVICES
  # ============================================================================
  
  # ü§ñ Multi-Agent System - Advanced agent coordination
  multi-agent-system:
    build:
      context: ./services/multi-agent-system
      dockerfile: Dockerfile
    container_name: multi-agent-system
    ports:
      - "8970:8970"
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - A2A_HUB_HOST=a2a-coordination-hub
      - A2A_HUB_PORT=8891
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      neural-thought-engine:
        condition: service_healthy
      a2a-coordination-hub:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8970/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üé≠ Consensus Manager - Decision consensus
  consensus-manager:
    build:
      context: ./services/consensus-manager
      dockerfile: Dockerfile
    container_name: consensus-manager
    ports:
      - "8978:8978"
    environment:
      - MULTI_AGENT_HOST=multi-agent-system
      - MULTI_AGENT_PORT=8970
      - SWARM_INTELLIGENCE_HOST=swarm-intelligence-engine
      - SWARM_INTELLIGENCE_PORT=8977
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      multi-agent-system:
        condition: service_healthy
      swarm-intelligence-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8978/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üåä Emergence Detector - Pattern emergence detection
  emergence-detector:
    build:
      context: ./services/emergence-detector
      dockerfile: Dockerfile
    container_name: emergence-detector
    ports:
      - "8979:8979"
    environment:
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - CONSENSUS_MANAGER_HOST=consensus-manager
      - CONSENSUS_MANAGER_PORT=8978
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      neural-thought-engine:
        condition: service_healthy
      consensus-manager:
        condition: service_healthy
    networks:
      - ultimate-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8979/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ü§ñ Phi-2 Ultrafast Engine - Advanced reasoning and coding
  phi2-ultrafast-engine:
    build:
      context: ./services/phi2-ultrafast
      dockerfile: Dockerfile
    container_name: phi2-ultrafast-engine
    ports:
      - "8893:8893"
    environment:
      - MODEL_NAME=microsoft/phi-2
      - INFERENCE_PORT=8893
      - CUDA_VISIBLE_DEVICES=0
      - TORCH_DEVICE=cuda
    volumes:
      - ./models:/app/models
      - ./cache:/app/cache
    networks:
      - ultimate-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8893/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # ============================================================================
  # üìä MONITORING & MANAGEMENT
  # ============================================================================
  
  # üéØ Ultimate Architecture Summary - System overview + V5 MATHEMATICAL ORCHESTRATION + LLM INTEGRATION
  ultimate-architecture-summary:
    build:
      context: .
      dockerfile: ./services/architecture-summary/Dockerfile
    container_name: ultimate-architecture-summary
    ports:
      - "9001:9001"  # Architecture Summary Port
    environment:
      - HIGH_RANK_ADAPTER_HOST=high-rank-adapter
      - HIGH_RANK_ADAPTER_PORT=9000
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
      - ENHANCED_EXECUTION_HOST=enhanced-execution-suite
      - ENHANCED_EXECUTION_PORT=8998
      - ENHANCED_FACT_CHECKER_HOST=enhanced-fact-checker
      - ENHANCED_FACT_CHECKER_PORT=8885
      # V5 Enhanced Mathematical Orchestrator Integration
      - V5_MATH_ORCHESTRATOR_HOST=v5-enhanced-math-orchestrator
      - V5_MATH_ORCHESTRATOR_PORT=8990
      - V5_MATHEMATICAL_MONITORING_HOST=v5-mathematical-monitoring
      - V5_MATHEMATICAL_MONITORING_PORT=3001
      - ENABLE_MATHEMATICAL_ARCHITECTURE_MONITORING=true
      - SYMPY_VERIFICATION_STATUS_MONITORING=true
      # LLM-Integrated Gap Detection Layer Integration
      - LLM_GAP_DETECTOR_HOST=llm-integrated-gap-detector
      - LLM_GAP_DETECTOR_PORT=8997
      - LLM_CHAT_INTERFACE_HOST=llm-chat-interface
      - LLM_CHAT_INTERFACE_PORT=8996
      - BACKGROUND_LORA_MANAGER_HOST=background-lora-manager
      - BACKGROUND_LORA_MANAGER_PORT=8994
      - LORA_GAP_GENERATOR_HOST=lora-gap-detection-generator
      - LORA_GAP_GENERATOR_PORT=8993
      - ENABLE_LLM_INTEGRATION_MONITORING=true
      - ENABLE_FAST_CHAT_MONITORING=true
      - ENABLE_BACKGROUND_LORA_MONITORING=true
      # Research Paper Generation Suite Integration
      - ENHANCED_RESEARCH_BACKEND_HOST=enhanced-research-backend
      - ENHANCED_RESEARCH_BACKEND_PORT=5000
      - AI_DETECTION_MITIGATOR_HOST=ai-detection-mitigator
      - AI_DETECTION_MITIGATOR_PORT=5001
      - RESEARCH_PAPER_FRONTEND_HOST=research-paper-frontend
      - RESEARCH_PAPER_FRONTEND_PORT=3005
      - INFINITE_ELABORATION_HOST=infinite-elaboration-engine
      - INFINITE_ELABORATION_PORT=5002
      - RESEARCH_DEMO_HOST=research-system-demo
      - RESEARCH_DEMO_PORT=5003
      - ENABLE_RESEARCH_PAPER_MONITORING=true
      - ENABLE_AI_HUMANIZATION_MONITORING=true
      - ENABLE_INFINITE_ELABORATION_MONITORING=true
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    depends_on:
      high-rank-adapter:
        condition: service_healthy
      meta-orchestration-controller:
        condition: service_healthy
      enhanced-execution-suite:
        condition: service_healthy
      enhanced-fact-checker:
        condition: service_healthy
      v5-enhanced-math-orchestrator:
        condition: service_healthy
      llm-integrated-gap-detector:
        condition: service_healthy
      enhanced-research-backend:
        condition: service_healthy
      ai-detection-mitigator:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./ultimate_ai_architecture_summary.py:/app/ultimate_ai_architecture_summary.py
      - ./v5_implementation_summary.md:/app/v5_mathematical_documentation.md
      - ./research_paper_integration_summary.md:/app/research_paper_documentation.md
      - ./llm_gap_detection_solution.md:/app/llm_integration_documentation.md
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================================================
  # üßÆ LAYER 4: V5 ENHANCED MATHEMATICAL ORCHESTRATOR - SYMPY VERIFICATION
  # ============================================================================
  
  # üßÆ V5 Enhanced Mathematical Orchestrator - SymPy Mathematical Verification Layer
  v5-enhanced-math-orchestrator:
    build:
      context: .
      dockerfile: Dockerfile.enhanced_math
    container_name: v5-enhanced-math-orchestrator
    ports:
      - "8990:8990"  # Enhanced Mathematical Processing Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - NEO4J_HOST=neo4j
      - NEO4J_PORT=7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=thinking123
      - OLLAMA_HOST=godlike-ollama
      - OLLAMA_PORT=11434
      # V5 Enhanced Mathematical Features
      - SYMPY_VERIFICATION=true
      - MATHEMATICAL_PRECISION_MODE=true
      - ARITHMETIC_ACCURACY_TARGET=100
      - ENABLE_REAL_TIME_CORRECTION=true
      - ENABLE_ENDPOINT_FALLBACKS=true
      # Integration with Ultimate Architecture
      - HIGH_RANK_ADAPTER_HOST=high-rank-adapter
      - HIGH_RANK_ADAPTER_PORT=9000
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
      - ENHANCED_EXECUTION_HOST=enhanced-execution-suite
      - ENHANCED_EXECUTION_PORT=8998
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      # Service Integration
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      - RAG_ORCHESTRATOR_HOST=rag-orchestrator
      - RAG_ORCHESTRATOR_PORT=8953
      - RAG_ROUTER_HOST=rag-router-enhanced
      - RAG_ROUTER_PORT=8951
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
      - MULTI_CONCEPT_DETECTOR_PORT=8860
      - ENHANCED_PROMPT_LORA_HOST=enhanced-prompt-lora
      - ENHANCED_PROMPT_LORA_PORT=8880
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - SWARM_INTELLIGENCE_HOST=swarm-intelligence-engine
      - SWARM_INTELLIGENCE_PORT=8977
    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      godlike-ollama:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
      rag-coordination-interface:
        condition: service_healthy
      multi-concept-detector:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./adaptive_confidence_orchestrator_v5_enhanced_math.py:/app/main.py
      - ./test_sympy_verification.py:/app/test_sympy_verification.py
      - ./v5_implementation_summary.md:/app/documentation.md
      - ./logs:/app/logs
      - ./test_results:/app/test_results
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8990/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 4G
        limits:
          cpus: '8.0'
          memory: 16G

  # üß™ SymPy Verification Tester - Mathematical Accuracy Testing
  sympy-verification-tester:
    build:
      context: .
      dockerfile: Dockerfile.test
    container_name: sympy-verification-tester
    environment:
      - ENHANCED_ORCHESTRATOR_URL=http://v5-enhanced-math-orchestrator:8990
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    volumes:
      - ./test_sympy_verification.py:/app/test_sympy_verification.py
      - ./test_results:/app/test_results
    networks:
      - ultimate-network
    depends_on:
      v5-enhanced-math-orchestrator:
        condition: service_healthy
    profiles:
      - testing
    restart: "no"

  # üìä V5 Mathematical Monitoring Dashboard - Enhanced Monitoring
  v5-mathematical-monitoring:
    build:
      context: .
      dockerfile: Dockerfile.monitoring
    container_name: v5-mathematical-monitoring
    ports:
      - "3001:3000"  # Mathematical Monitoring Port (offset to avoid conflicts)
    environment:
      - ORCHESTRATOR_URL=http://v5-enhanced-math-orchestrator:8990
      - HIGH_RANK_ADAPTER_URL=http://high-rank-adapter:9000
      - META_ORCHESTRATION_URL=http://meta-orchestration-controller:8999
      - ENHANCED_EXECUTION_URL=http://enhanced-execution-suite:8998
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
    networks:
      - ultimate-network
    depends_on:
      v5-enhanced-math-orchestrator:
        condition: service_healthy
    restart: unless-stopped

  # ============================================================================
  # üöÄ COMPREHENSIVE TEST SUITE - 100+ Questions Multi-LLM Testing
  # ============================================================================
  
  # Comprehensive Test Suite - 100+ Questions Multi-LLM Testing
  comprehensive-test-suite:
    build:
      context: .
      dockerfile: Dockerfile.comprehensive_test
    container_name: comprehensive-test-suite
    environment:
      - ORCHESTRATOR_URL=http://v5-enhanced-math-orchestrator:8990
      - HIGH_RANK_ADAPTER_URL=http://high-rank-adapter:9000
      - META_ORCHESTRATION_URL=http://meta-orchestration-controller:8999
      - ENHANCED_EXECUTION_URL=http://enhanced-execution-suite:8998
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Comprehensive Testing Configuration
      - ENABLE_BABY_MATH_TESTS=true
      - ENABLE_COMPLEX_MATH_TESTS=true
      - ENABLE_ZEBRA_PUZZLES=true
      - ENABLE_UNANSWERABLE_QUESTIONS=true
      - ENABLE_CREATIVE_REASONING=true
      - ENABLE_SCIENTIFIC_QUERIES=true
      - ENABLE_PARADOXES=true
      - ENABLE_WORD_PROBLEMS=true
      - ENABLE_IMPOSSIBLE_SCENARIOS=true
      - TEST_OUTPUT_FORMAT=json
      - PARALLEL_TESTING=true
      # LLM Models for Testing
      - TEST_LLAMA_1B=true
      - TEST_MISTRAL_7B=true
      - TEST_PHI3_MINI=true
    volumes:
      - ./comprehensive_test_suite_v5.py:/app/comprehensive_test_suite_v5.py
      - ./test_results:/app/test_results
      - ./reports:/app/reports
    networks:
      - ultimate-network
    depends_on:
      v5-enhanced-math-orchestrator:
        condition: service_healthy
      high-rank-adapter:
        condition: service_healthy
      meta-orchestration-controller:
        condition: service_healthy
      enhanced-execution-suite:
        condition: service_healthy
    profiles:
      - comprehensive-testing
    restart: "no"

  # üîç Enhanced Fact-Checker with Web Search - SOLVES FALSE STATEMENT PROBLEM
  enhanced-fact-checker-web:
    build:
      context: ./services/enhanced-fact-checker-web
      dockerfile: Dockerfile
    container_name: enhanced-fact-checker-web-search
    ports:
      - "8886:8886"  # Enhanced Web-Search Fact-Checking Port
    environment:
      - RESEARCH_BACKEND_HOST=enhanced-research-backend
      - RESEARCH_BACKEND_PORT=5000
      - AI_DETECTION_HOST=ai-detection-mitigator
      - AI_DETECTION_PORT=5001
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - WEB_SEARCH_CACHE_TTL=3600
      - FACT_CHECK_CONFIDENCE_THRESHOLD=0.6
      - MAX_CLAIMS_PER_REQUEST=50
    depends_on:
      - redis
      - enhanced-research-backend
    volumes:
      - ./logs/enhanced-fact-checker-web:/app/logs
      - ./cache/web-search:/app/cache
    networks:
      - ultimate-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8886/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s

  # ============================================================================
  # üß† V7 BASE LOGIC AGENT - QUANTUM-READY FOUNDATION WITH EINSTEIN PUZZLE SOLVER
  # ============================================================================
  
  # ü§ñ V7 Base Logic Agent - Pure Computational Intelligence + Einstein Puzzle Solver
  v7-base-logic-agent:
    build:
      context: .
      dockerfile: ./services/v7-base-logic-agent/Dockerfile
    container_name: v7-base-logic-agent
    ports:
      - "8991:8991"  # V7 Base Logic Agent Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # V7 Enhanced Capabilities
      - ENABLE_EINSTEIN_PUZZLE_SOLVER=true
      - ENABLE_CONSTRAINT_SATISFACTION=true
      - ENABLE_AI_FAILSAFE_TRAPS=true
      - ENABLE_SELF_CORRECTION=true
      - CLASSIFICATION_ACCURACY_TARGET=91.7
      - EINSTEIN_SOLVER_SUCCESS_TARGET=100.0
      - MATHEMATICAL_PRECISION_TARGET=100.0
      # Advanced Logic Features
      - MULTI_ATTRIBUTE_CONSTRAINT_MATRIX=true
      - FORWARD_CHECKING_ALGORITHM=true
      - CONTEXTUAL_PATTERN_MATCHING=true
      - ZERO_FALSE_POSITIVES_MODE=true
      - QUANTUM_INTEGRATION_READY=true
      # Integration with Ultimate Architecture
      - HIGH_RANK_ADAPTER_HOST=high-rank-adapter
      - HIGH_RANK_ADAPTER_PORT=9000
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
      - ENHANCED_EXECUTION_HOST=enhanced-execution-suite
      - ENHANCED_EXECUTION_PORT=8998
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - V5_MATH_ORCHESTRATOR_HOST=v5-enhanced-math-orchestrator
      - V5_MATH_ORCHESTRATOR_PORT=8990
    depends_on:
      redis:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
      high-rank-adapter:
        condition: service_healthy
      meta-orchestration-controller:
        condition: service_healthy
      enhanced-execution-suite:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./base_logic_agent_v7.py:/app/base_logic_agent_v7.py
      - ./test_classification_fix.py:/app/tests/test_classification_fix.py
      - ./test_einstein_puzzle.py:/app/tests/test_einstein_puzzle.py
      - ./test_logic_puzzles_v7.py:/app/tests/test_logic_puzzles_v7.py
      - ./v7_logic_agent_logs:/app/logs
    labels:
      - "service.type=base-logic-agent"
      - "service.version=v7.1.0"
      - "service.capabilities=einstein-puzzle-solver,constraint-satisfaction,ai-failsafe,quantum-ready"
      - "service.classification_accuracy=91.7%"
      - "service.einstein_solver_success=100%"
      - "service.mathematical_precision=100%"
      - "service.integration=ultimate-architecture-ready"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8991/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 2G
        limits:
          cpus: '3.0'
          memory: 4G

  # üß™ V7 Logic Agent Testing Suite - Comprehensive Testing
  v7-logic-agent-tester:
    build:
      context: .
      dockerfile: ./services/v7-logic-tester/Dockerfile
    container_name: v7-logic-agent-tester
    environment:
      - V7_LOGIC_AGENT_HOST=v7-base-logic-agent
      - V7_LOGIC_AGENT_PORT=8991
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Testing Configuration
      - ENABLE_CLASSIFICATION_TESTS=true
      - ENABLE_EINSTEIN_PUZZLE_TESTS=true
      - ENABLE_LOGIC_PUZZLE_TESTS=true
      - ENABLE_MATHEMATICAL_TESTS=true
      - ENABLE_AI_FAILSAFE_TESTS=true
      - TARGET_CLASSIFICATION_ACCURACY=91.7
      - TARGET_EINSTEIN_SUCCESS_RATE=100.0
      - TARGET_MATHEMATICAL_PRECISION=100.0
    depends_on:
      v7-base-logic-agent:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./test_classification_fix.py:/app/test_classification_fix.py
      - ./test_einstein_puzzle.py:/app/test_einstein_puzzle.py
      - ./test_logic_puzzles_v7.py:/app/test_logic_puzzles_v7.py
      - ./v7_test_results:/app/test_results
      - ./v7_test_reports:/app/reports
    labels:
      - "service.type=v7-logic-testing"
      - "service.purpose=comprehensive-validation"
      - "service.test_categories=classification,einstein-puzzles,logic-puzzles,mathematical,ai-failsafe"
    profiles:
      - v7-testing
    restart: "no"

  # ============================================================================
  # ‚ö° LLM-INTEGRATED GAP DETECTION LAYER - FAST CHAT WITH BACKGROUND LORA
  # ============================================================================
  
  # üöÄ LLM-Integrated Gap Detector - Fast Chat with Smart Gap Detection
  llm-integrated-gap-detector:
    build:
      context: .
      dockerfile: ./services/llm-gap-detection/Dockerfile
    container_name: llm-integrated-gap-detector
    ports:
      - "8997:8997"  # LLM-Integrated Gap Detection Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - OLLAMA_HOST=godlike-ollama
      - OLLAMA_PORT=11434
      - RAG_SERVICE_HOST=rag-cpu-optimized
      - RAG_SERVICE_PORT=8902
      # Integration with Ultimate Architecture
      - HIGH_RANK_ADAPTER_HOST=high-rank-adapter
      - HIGH_RANK_ADAPTER_PORT=9000
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
      - ENHANCED_EXECUTION_HOST=enhanced-execution-suite
      - ENHANCED_EXECUTION_PORT=8998
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      # LoRA Integration for Background Training
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - ENHANCED_PROMPT_LORA_HOST=enhanced-prompt-lora
      - ENHANCED_PROMPT_LORA_PORT=8880
      - CONCEPT_TRAINING_HOST=concept-training-worker
      - CONCEPT_TRAINING_PORT=8851
      - ENHANCED_CRAWLER_HOST=enhanced-crawler-nlp
      - ENHANCED_CRAWLER_PORT=8850
      # LLM Gap Detection Configuration
      - ENABLE_INSTANT_RESPONSES=true
      - ENABLE_FAST_CHAT_MODE=true
      - ENABLE_BACKGROUND_LORA_CREATION=true
      - INSTANT_CONFIDENCE_THRESHOLD=0.70
      - FAST_CONFIDENCE_THRESHOLD=0.60
      - MAX_CHAT_RESPONSE_TIME=30
      - BACKGROUND_LORA_TRAINING_TIME=300
      # Field Expertise Configuration
      - AI_FIELD_EXPERTISE=0.85
      - HEALTHCARE_AI_EXPERTISE=0.65
      - QUANTUM_COMPUTING_EXPERTISE=0.45
      - RENEWABLE_ENERGY_EXPERTISE=0.60
      - CYBERSECURITY_EXPERTISE=0.70
    depends_on:
      redis:
        condition: service_healthy
      godlike-ollama:
        condition: service_healthy
      enhanced-execution-suite:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
      lora-coordination-hub:
        condition: service_healthy
      rag-cpu-optimized:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./llm_integrated_gap_detection.py:/app/llm_integrated_gap_detection.py
      - ./lora_gap_detection_generator.py:/app/lora_gap_detection_generator.py
      - ./background_lora_tasks:/app/background_tasks
      - ./llm_gap_logs:/app/logs
    labels:
      - "service.type=llm-gap-detection"
      - "service.capabilities=instant-responses,fast-chat,background-lora,smart-routing"
      - "service.response_modes=instant,fast,thorough,research"
      - "service.integration=ultimate-architecture-enhanced"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8997/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.5'
          memory: 3G
        limits:
          cpus: '4.0'
          memory: 8G

  # üí¨ LLM Chat Interface - Easy Integration for Chat Applications
  llm-chat-interface:
    build:
      context: .
      dockerfile: ./services/llm-chat-interface/Dockerfile
    container_name: llm-chat-interface
    ports:
      - "8996:8996"  # LLM Chat Interface Port (changed from quality-adapter-manager)
    environment:
      - LLM_GAP_DETECTOR_HOST=llm-integrated-gap-detector
      - LLM_GAP_DETECTOR_PORT=8997
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Chat Interface Configuration
      - ENABLE_CONVERSATION_HISTORY=true
      - MAX_CONVERSATION_HISTORY=20
      - ENABLE_REAL_TIME_STATUS=true
      - ENABLE_BACKGROUND_TASK_MONITORING=true
      - CHAT_SESSION_TIMEOUT=3600
      # Integration Features
      - ENABLE_MODE_SELECTION=true
      - ENABLE_FIELD_DETECTION=true
      - ENABLE_CONFIDENCE_REPORTING=true
      - ENABLE_GAP_ANALYSIS_REPORTING=true
    depends_on:
      llm-integrated-gap-detector:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./chat_demo.py:/app/chat_demo.py
      - ./conversation_sessions:/app/sessions
      - ./chat_interface_logs:/app/logs
    labels:
      - "service.type=llm-chat-interface"
      - "service.features=conversation-history,real-time-status,mode-selection"
      - "service.purpose=easy-llm-integration"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8996/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # üîÑ Background LoRA Manager - Non-blocking LoRA Creation
  background-lora-manager:
    build:
      context: .
      dockerfile: ./services/background-lora/Dockerfile
    container_name: background-lora-manager
    ports:
      - "8994:8994"  # Background LoRA Manager Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - LLM_GAP_DETECTOR_HOST=llm-integrated-gap-detector
      - LLM_GAP_DETECTOR_PORT=8997
      # LoRA Infrastructure Integration
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - CONCEPT_TRAINING_HOST=concept-training-worker
      - CONCEPT_TRAINING_PORT=8851
      - ENHANCED_CRAWLER_HOST=enhanced-crawler-nlp
      - ENHANCED_CRAWLER_PORT=8850
      - ENHANCED_PROMPT_LORA_HOST=enhanced-prompt-lora
      - ENHANCED_PROMPT_LORA_PORT=8880
      # Original Gap Detection Integration
      - LORA_GAP_DETECTION_HOST=lora-gap-detection-generator
      - LORA_GAP_DETECTION_PORT=8993
      # Background Processing Configuration
      - MAX_CONCURRENT_LORA_TRAINING=3
      - BACKGROUND_TRAINING_TIMEOUT=600
      - ENABLE_TRAINING_PROGRESS_MONITORING=true
      - ENABLE_AUTOMATIC_CLEANUP=true
      - TRAINING_QUEUE_MAX_SIZE=50
    depends_on:
      llm-integrated-gap-detector:
        condition: service_healthy
      lora-coordination-hub:
        condition: service_healthy
      concept-training-worker:
        condition: service_healthy
      enhanced-crawler-nlp:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./background_lora_tasks:/app/tasks
      - ./background_lora_logs:/app/logs
      - ./lora_training_cache:/app/cache
    labels:
      - "service.type=background-lora-manager"
      - "service.capabilities=non-blocking-training,progress-monitoring,queue-management"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8994/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 2G
        limits:
          cpus: '3.0'
          memory: 6G

  # üéØ Original LoRA Gap Detection Generator - Research Paper Mode
  lora-gap-detection-generator:
    build:
      context: .
      dockerfile: ./services/lora-gap-generator/Dockerfile
    container_name: lora-gap-detection-generator
    ports:
      - "8993:8993"  # Original Gap Detection Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # All existing LoRA infrastructure connections
      - ENHANCED_CRAWLER_HOST=enhanced-crawler-nlp
      - ENHANCED_CRAWLER_PORT=8850
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
      - MULTI_CONCEPT_DETECTOR_PORT=8860
      - CONCEPT_TRAINING_HOST=concept-training-worker
      - CONCEPT_TRAINING_PORT=8851
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - ENHANCED_PROMPT_LORA_HOST=enhanced-prompt-lora
      - ENHANCED_PROMPT_LORA_PORT=8880
      - OPTIMAL_LORA_ROUTER_HOST=optimal-lora-router
      - OPTIMAL_LORA_ROUTER_PORT=5030
      # Research Mode Configuration (Original Behavior)
      - ENABLE_FULL_LORA_CREATION=true
      - ENABLE_GAP_DETECTION_POLLING=true
      - MAX_LORA_WAIT_TIME=600  # 10 minutes
      - TRAINING_EXAMPLES_TARGET=150
      - QUALITY_TARGET=9.0
    depends_on:
      enhanced-crawler-nlp:
        condition: service_healthy
      multi-concept-detector:
        condition: service_healthy
      concept-training-worker:
        condition: service_healthy
      lora-coordination-hub:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./lora_gap_detection_generator.py:/app/lora_gap_detection_generator.py
      - ./gap_detection_papers:/app/papers
      - ./lora_gap_logs:/app/logs
    labels:
      - "service.type=lora-gap-detection-original"
      - "service.purpose=research-paper-generation"
      - "service.mode=thorough-lora-creation"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8993/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 4G
        limits:
          cpus: '6.0'
          memory: 12G

  # ============================================================================
  # ü§ñ ULTIMATE CHAT ORCHESTRATOR - THE MAIN CHAT BOT THAT CALLS ALL OTHER BOTS
  # ============================================================================
  
  # üéØ Ultimate Chat Orchestrator - Main chat bot with heavy A2A interaction
  ultimate-chat-orchestrator:
    build:
      context: .
      dockerfile: Dockerfile.ultimate-chat
    container_name: ultimate-chat-orchestrator
    ports:
      - "8951:8951"  # Ultimate Chat Orchestrator Port (changed to avoid conflict)
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # All Agent Endpoints Integration
      - V7_LOGIC_HOST=v7-base-logic-agent
      - A2A_ORCHESTRATOR_HOST=a2a-coordination-hub
      - RESEARCH_AGENT_HOST=enhanced-research-agent-v3
      - QUANTUM_COLLABORATOR_HOST=collaborative-quantum-agent
      - MATHEMATICAL_SPECIALIST_HOST=v5-enhanced-math-orchestrator
      - PHYSICS_SPECIALIST_HOST=neural-thought-engine
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - MCP_TRANSCRIPT_HOST=transcript-ingest
      - ENHANCED_CRAWLER_HOST=enhanced-crawler-nlp-intelligent
      - CONCEPT_TRAINING_HOST=concept-training-worker-enhanced
      - INTELLIGENT_RETRAINING_HOST=intelligent-retraining-coordinator
      # Performance and monitoring
      - ENABLE_CONVERSATION_AWARENESS=true
      - ENABLE_ERROR_DETECTION=true
      - ENABLE_HEAVY_A2A_LAYERS=true
      - ENABLE_LORA_INTEGRATION=true
      - ENABLE_RESEARCH_INTEGRATION=true
      - ENABLE_QUANTUM_ENHANCEMENT=true
      - ENABLE_MULTI_AGENT_COORDINATION=true
      - SLIP_DETECTION_THRESHOLD=0.8
      - ERROR_CORRECTION_ENABLED=true
      - SESSION_MANAGEMENT_ENABLED=true
      - REAL_TIME_LEARNING_ENABLED=true
    depends_on:
      - redis
      - v7-base-logic-agent
      - a2a-coordination-hub
      - enhanced-research-agent-v3
      - lora-coordination-hub
      - rag-coordination-interface
      - enhanced-crawler-nlp
      - concept-training-worker
      - intelligent-retraining-coordinator
    restart: unless-stopped
    networks:
      - ultimate-network

  # ============================================================================
  # üß†üîÑ INTELLIGENT RETRAINING SYSTEM - AUTO-QUALITY ASSESSMENT & LORA CREATION
  # ============================================================================
  
  # üï∏Ô∏è Enhanced Crawler NLP - Intelligent Filtering
  enhanced-crawler-nlp-intelligent:
    build:
      context: ./services/enhanced-crawler-nlp
      dockerfile: Dockerfile
    container_name: enhanced-crawler-nlp-intelligent
    ports:
      - "8852:8852"  # Enhanced Crawler NLP Port (changed to avoid conflict)
    environment:
      - PORT=8852
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # ArXiv configuration
      - ARXIV_API_BASE=http://export.arxiv.org/api/query
      - ARXIV_CATEGORIES=cs.AI,cs.LG,cs.CL,cs.CV,stat.ML,physics.comp-ph
      - ARXIV_MAX_RESULTS_PER_QUERY=50
      - ARXIV_QUERY_INTERVAL_SECONDS=300
      - ENABLE_ARXIV_CRAWLING=true
      # NLP filtering thresholds
      - NLP_RELEVANCE_THRESHOLD=0.7
      - TOPIC_DIVERSITY_THRESHOLD=0.6
      - QUALITY_SCORE_THRESHOLD=0.8
      - NOVELTY_DETECTION_THRESHOLD=0.5
      # Auto-selector configuration
      - AUTO_SELECTOR_MIN_QUALITY=8.0
      - AUTO_SELECTOR_MIN_RELEVANCE=0.75
      - AUTO_SELECTOR_MIN_CITATIONS=10
      - AUTO_SELECTOR_MAX_AGE_DAYS=365
      # Service integration
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - CONCEPT_TRAINING_HOST=concept-training-worker-enhanced
      - INTELLIGENT_RETRAINING_HOST=intelligent-retraining-coordinator
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - ultimate-network

  # üß† Concept Training Worker - Enhanced Auto-Quality Assessment
  concept-training-worker-enhanced:
    build:
      context: ./services/concept-training-worker
      dockerfile: Dockerfile
    container_name: concept-training-worker-enhanced
    ports:
      - "8853:8853"  # Concept Training Worker Port (changed to avoid conflict)
    environment:
      - PORT=8853
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Enhanced training configuration
      - LORA_RANK=16
      - LORA_ALPHA=32
      - LORA_DROPOUT=0.1
      - TRAINING_BATCH_SIZE=8
      - LEARNING_RATE=2e-4
      - MAX_TRAINING_EPOCHS=10
      # Auto-selector configuration
      - AUTO_SELECTOR_ENABLED=true
      - AUTO_APPROVE_HIGH_QUALITY=true
      - HIGH_QUALITY_AUTO_THRESHOLD=9.0
      # Quality thresholds
      - TRAINING_QUALITY_THRESHOLD=8.0
      - VALIDATION_ACCURACY_THRESHOLD=0.85
      - CONCEPT_COHERENCE_THRESHOLD=0.8
      - DOMAIN_SPECIFICITY_THRESHOLD=0.7
      # Service integration
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - ENHANCED_CRAWLER_HOST=enhanced-crawler-nlp-intelligent
      - INTELLIGENT_RETRAINING_HOST=intelligent-retraining-coordinator
    depends_on:
      - redis
      - enhanced-crawler-nlp-intelligent
      - lora-coordination-hub
    restart: unless-stopped
    networks:
      - ultimate-network

  # üß†üîÑ Intelligent Retraining Coordinator - Pipeline Orchestration
  intelligent-retraining-coordinator:
    build:
      context: .
      dockerfile: Dockerfile.intelligent-retraining
    container_name: intelligent-retraining-coordinator
    ports:
      - "8849:8849"  # Intelligent Retraining Coordinator Port
    environment:
      - PORT=8849
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Coordinator configuration
      - RETRAINING_SCHEDULE_HOURS=24
      - PERFORMANCE_CHECK_INTERVAL_MINUTES=30
      - MIN_CONTENT_FOR_TRAINING=5
      - QUALITY_DECLINE_THRESHOLD=0.1
      - AUTO_RETRAINING_ENABLED=true
      - MAX_CONCURRENT_CYCLES=3
      # Service integration
      - ENHANCED_CRAWLER_HOST=enhanced-crawler-nlp-intelligent
      - CONCEPT_TRAINING_HOST=concept-training-worker-enhanced
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - ULTIMATE_CHAT_HOST=ultimate-chat-orchestrator
      - MULTI_CONCEPT_DETECTOR_HOST=multi-concept-detector
    depends_on:
      - redis
      - enhanced-crawler-nlp-intelligent
      - concept-training-worker-enhanced
      - lora-coordination-hub
    restart: unless-stopped
    networks:
      - ultimate-network

  # üöÄ Enhanced Research Agent v3 - Ultimate Knowledge Integration
  enhanced-research-agent-v3:
    build:
      context: ./services/enhanced-research-agent-v3
      dockerfile: Dockerfile
    container_name: enhanced-research-agent-v3
    ports:
      - "8999:8999"  # Enhanced Research Agent v3 API
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      - LLM_GAP_DETECTOR_URL=http://llm-integrated-gap-detector:8997
      - RAG_2025_URL=http://rag-coordination-interface:8952
      - BACKGROUND_LORA_URL=http://background-lora-manager:8994
      - ENHANCED_CRAWLER_URL=http://enhanced-crawler-nlp:8850
      - FACT_CHECKER_URL=http://enhanced-fact-checker:8885
      # Research Agent Configuration
      - MAX_LORA_WAIT_MINUTES=15
      - ENABLE_RAG_CRAWLING=true
      - ENABLE_ANTI_HALLUCINATION=true
      - DEFAULT_QUALITY_THRESHOLD=8.5
      - DEFAULT_TARGET_QUALITY=9.0
      - ENABLE_META_LEARNING=true
      - ENABLE_AUTONOMOUS_FOLLOW_UP=false
      # Integration with Ultimate AI Orchestration Architecture
      - NEURAL_THOUGHT_ENGINE_URL=http://neural-thought-engine:8890
      - META_ORCHESTRATION_URL=http://meta-orchestration-controller:8999
      - ENHANCED_EXECUTION_URL=http://enhanced-execution-suite:8998
      - V7_BASE_LOGIC_URL=http://v7-base-logic-agent:8991
      - QUANTUM_AGENT_URL=http://collaborative-quantum-agent:8975
    depends_on:
      redis:
        condition: service_healthy
      llm-integrated-gap-detector:
        condition: service_healthy
      rag-coordination-interface:
        condition: service_healthy
      background-lora-manager:
        condition: service_healthy
      enhanced-crawler-nlp:
        condition: service_healthy
      enhanced-fact-checker:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./enhanced_research_agent_v3.py:/app/enhanced_research_agent_v3.py
      - ./logs/research_agent:/app/logs
    labels:
      - "service.type=enhanced-research-agent-v3"
      - "service.capabilities=15min-lora,rag-circular-growth,anti-hallucination,meta-learning"
      - "service.architecture-layer=knowledge-integration"
      - "service.purpose=ultimate-research-intelligence"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8999/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 4G
        limits:
          cpus: '8.0'
          memory: 16G

  # üß†üí° Confidence-Driven LoRA Creator - Automatic Knowledge Gap Detection and LoRA Creation
  confidence-driven-lora-creator:
    build:
      context: .
      dockerfile: Dockerfile.confidence-driven-lora
    container_name: confidence-driven-lora-creator
    ports:
      - "8848:8848"  # Confidence-Driven LoRA Creator Port
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Integration with existing services
      - ENHANCED_CRAWLER_HOST=enhanced-crawler-nlp
      - ENHANCED_CRAWLER_PORT=8850
      - CONCEPT_TRAINING_HOST=concept-training-worker
      - CONCEPT_TRAINING_PORT=8851
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - RETRAINING_COORDINATOR_HOST=intelligent-retraining-coordinator
      - RETRAINING_COORDINATOR_PORT=8849
      - LLM_GAP_DETECTOR_HOST=llm-integrated-gap-detector
      - LLM_GAP_DETECTOR_PORT=8997
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      # Confidence system configuration
      - LORA_CREATION_THRESHOLD=0.3
      - URGENT_LORA_THRESHOLD=0.1
      - GAP_DETECTION_THRESHOLD=0.5
      - ENABLE_REAL_TIME_MONITORING=true
      - ENABLE_AUTOMATIC_LEARNING=true
      - MAX_CONCURRENT_LORA_REQUESTS=5
      # Integration with Ultimate Architecture
      - HIGH_RANK_ADAPTER_HOST=high-rank-adapter
      - HIGH_RANK_ADAPTER_PORT=9000
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
      - ENHANCED_EXECUTION_HOST=enhanced-execution-suite
      - ENHANCED_EXECUTION_PORT=8998
    depends_on:
      redis:
        condition: service_healthy
      enhanced-crawler-nlp:
        condition: service_healthy
      concept-training-worker:
        condition: service_healthy
      lora-coordination-hub:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./confidence_driven_lora_creator.py:/app/confidence_driven_lora_creator.py
      - ./requirements.txt:/app/requirements.txt
      - ./logs/confidence_lora:/app/logs
      - ./knowledge_gaps:/app/knowledge_gaps
      - ./confidence_patterns:/app/confidence_patterns
      - ./lora_requests:/app/lora_requests
    labels:
      - "service.type=confidence-driven-lora-creator"
      - "service.capabilities=gap-detection,automatic-lora-creation,uncertainty-monitoring,confidence-assessment"
      - "service.learning_triggers=explicit-uncertainty,low-confidence,gap-frequency"
      - "service.integration=ultimate-chat-orchestrator,retraining-coordinator,lora-pipeline"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8848/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.0'
          memory: 2G
        limits:
          cpus: '3.0'
          memory: 6G

  # ü§ñüí° Ultimate Chat Orchestrator with Confidence - Enhanced with Knowledge Gap Detection
  ultimate-chat-orchestrator-with-confidence:
    build:
      context: .
      dockerfile: Dockerfile.ultimate-chat-confidence
    container_name: ultimate-chat-orchestrator-with-confidence
    ports:
      - "8950:8950"  # Ultimate Chat Orchestrator Port (Enhanced)
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Confidence-driven integration
      - CONFIDENCE_LORA_CREATOR_HOST=confidence-driven-lora-creator
      - CONFIDENCE_LORA_CREATOR_PORT=8848
      # All existing agent endpoints
      - LLM_GAP_DETECTOR_HOST=llm-integrated-gap-detector
      - LLM_GAP_DETECTOR_PORT=8997
      - NEURAL_ENGINE_HOST=neural-thought-engine
      - NEURAL_ENGINE_PORT=8890
      - RAG_COORDINATION_HOST=rag-coordination-interface
      - RAG_COORDINATION_PORT=8952
      - LORA_COORDINATION_HOST=lora-coordination-hub
      - LORA_COORDINATION_PORT=8995
      - ENHANCED_FACT_CHECKER_HOST=enhanced-fact-checker
      - ENHANCED_FACT_CHECKER_PORT=8885
      - V7_LOGIC_AGENT_HOST=v7-base-logic-agent
      - V7_LOGIC_AGENT_PORT=8991
      - QUANTUM_AGENT_HOST=collaborative-quantum-agent
      - QUANTUM_AGENT_PORT=8975
      # Confidence configuration
      - LOW_CONFIDENCE_THRESHOLD=0.4
      - VERY_LOW_CONFIDENCE_THRESHOLD=0.2
      - UNCERTAINTY_TRIGGER_THRESHOLD=0.3
      - ENABLE_AUTOMATIC_LORA=true
      - ENABLE_TRANSPARENCY=true
      - MAX_LEARNING_ATTEMPTS_PER_TOPIC=3
      # Integration with Ultimate AI Orchestration Architecture
      - HIGH_RANK_ADAPTER_HOST=high-rank-adapter
      - HIGH_RANK_ADAPTER_PORT=9000
      - META_ORCHESTRATION_HOST=meta-orchestration-controller
      - META_ORCHESTRATION_PORT=8999
      - ENHANCED_EXECUTION_HOST=enhanced-execution-suite
      - ENHANCED_EXECUTION_PORT=8998
    depends_on:
      redis:
        condition: service_healthy
      confidence-driven-lora-creator:
        condition: service_healthy
      llm-integrated-gap-detector:
        condition: service_healthy
      neural-thought-engine:
        condition: service_healthy
      rag-coordination-interface:
        condition: service_healthy
      lora-coordination-hub:
        condition: service_healthy
      enhanced-fact-checker:
        condition: service_healthy
      v7-base-logic-agent:
        condition: service_healthy
      collaborative-quantum-agent:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./ultimate_chat_orchestrator_with_confidence.py:/app/ultimate_chat_orchestrator_with_confidence.py
      - ./requirements.txt:/app/requirements.txt
      - ./logs/chat_orchestrator:/app/logs
      - ./conversations:/app/conversations
      - ./confidence_sessions:/app/confidence_sessions
      - ./user_analytics:/app/user_analytics
    labels:
      - "service.type=ultimate-chat-orchestrator-enhanced"
      - "service.capabilities=multi-agent-coordination,confidence-monitoring,automatic-learning,gap-detection"
      - "service.confidence_features=real-time-assessment,uncertainty-detection,lora-triggering,transparency"
      - "service.integration=confidence-driven-lora-creator,ultimate-architecture"
      - "service.learning_modes=reactive-gap-filling,proactive-improvement,conversation-aware"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8950/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '1.5'
          memory: 3G
        limits:
          cpus: '4.0'
          memory: 8G

  # üß™ Confidence-Driven System Tester - Comprehensive Testing Suite
  confidence-driven-system-tester:
    build:
      context: .
      dockerfile: Dockerfile.confidence-system-tester
    container_name: confidence-driven-system-tester
    environment:
      - CONFIDENCE_LORA_CREATOR_HOST=confidence-driven-lora-creator
      - CONFIDENCE_LORA_CREATOR_PORT=8848
      - ULTIMATE_CHAT_HOST=ultimate-chat-orchestrator-with-confidence
      - ULTIMATE_CHAT_PORT=8950
      - ENHANCED_CRAWLER_HOST=enhanced-crawler-nlp
      - ENHANCED_CRAWLER_PORT=8850
      - CONCEPT_TRAINING_HOST=concept-training-worker
      - CONCEPT_TRAINING_PORT=8851
      - RETRAINING_COORDINATOR_HOST=intelligent-retraining-coordinator
      - RETRAINING_COORDINATOR_PORT=8849
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Testing configuration
      - ENABLE_COMPREHENSIVE_TESTS=true
      - TEST_CONFIDENCE_ASSESSMENT=true
      - TEST_UNCERTAINTY_DETECTION=true
      - TEST_LORA_TRIGGERING=true
      - TEST_GAP_TRACKING=true
      - TEST_END_TO_END_PIPELINE=true
      - TEST_OUTPUT_FORMAT=json
      - PARALLEL_TESTING=true
    depends_on:
      confidence-driven-lora-creator:
        condition: service_healthy
      ultimate-chat-orchestrator-with-confidence:
        condition: service_healthy
      enhanced-crawler-nlp:
        condition: service_healthy
      concept-training-worker:
        condition: service_healthy
    networks:
      - ultimate-network
    volumes:
      - ./test_confidence_driven_system.py:/app/test_confidence_driven_system.py
      - ./confidence_demo.py:/app/confidence_demo.py
      - ./requirements.txt:/app/requirements.txt
      - ./test_results/confidence_system:/app/test_results
      - ./reports/confidence_system:/app/reports
      - ./logs/confidence_testing:/app/logs
    labels:
      - "service.type=confidence-system-testing"
      - "service.test_categories=confidence-assessment,uncertainty-detection,lora-triggering,gap-tracking,end-to-end"
      - "service.purpose=comprehensive-validation"
    profiles:
      - confidence-testing
    restart: "no"

  # üöÄ Confidence-Driven System Deployer - Automated Deployment and Validation
  confidence-driven-system-deployer:
    build:
      context: .
      dockerfile: Dockerfile.confidence-system-deployer
    container_name: confidence-driven-system-deployer
    environment:
      - DOCKER_HOST=unix:///var/run/docker.sock
      - CONFIDENCE_LORA_CREATOR_HOST=confidence-driven-lora-creator
      - CONFIDENCE_LORA_CREATOR_PORT=8848
      - ULTIMATE_CHAT_HOST=ultimate-chat-orchestrator-with-confidence
      - ULTIMATE_CHAT_PORT=8950
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=02211998
      # Deployment configuration
      - ENABLE_PREREQUISITES_CHECK=true
      - ENABLE_FILE_VALIDATION=true
      - ENABLE_HEALTH_MONITORING=true
      - ENABLE_INTEGRATION_VALIDATION=true
      - DEPLOYMENT_TIMEOUT_MINUTES=10
      - VALIDATION_RETRIES=5
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./deploy_confidence_driven_system.py:/app/deploy_confidence_driven_system.py
      - ./docker-compose-v10-ultimate.yml:/app/docker-compose.yml
      - ./Dockerfile.confidence-driven-lora:/app/Dockerfile.confidence-driven-lora
      - ./Dockerfile.ultimate-chat-confidence:/app/Dockerfile.ultimate-chat-confidence
      - ./confidence_driven_lora_creator.py:/app/confidence_driven_lora_creator.py
      - ./ultimate_chat_orchestrator_with_confidence.py:/app/ultimate_chat_orchestrator_with_confidence.py
      - ./requirements.txt:/app/requirements.txt
      - ./deployment_reports:/app/reports
      - ./logs/deployment:/app/logs
    networks:
      - ultimate-network
    labels:
      - "service.type=confidence-system-deployment"
      - "service.capabilities=automated-deployment,health-monitoring,integration-validation"
      - "service.purpose=system-orchestration"
    profiles:
      - confidence-deployment
    restart: "no"

  # üéØ Confidence Demo Service - Standalone Demonstration
  confidence-demo-service:
    build:
      context: .
      dockerfile: Dockerfile.confidence-demo
    container_name: confidence-demo-service
    ports:
      - "8847:8847"  # Demo Service Port
    environment:
      - DEMO_PORT=8847
      - STANDALONE_MODE=true
      - ENABLE_DETAILED_LOGGING=true
      - DEMO_QUERIES_COUNT=10
      - SIMULATION_MODE=true
    networks:
      - ultimate-network
    volumes:
      - ./confidence_demo.py:/app/confidence_demo.py
      - ./requirements.txt:/app/requirements.txt
      - ./demo_results:/app/demo_results
      - ./logs/confidence_demo:/app/logs
    labels:
      - "service.type=confidence-demo"
      - "service.purpose=standalone-demonstration"
      - "service.features=confidence-assessment,gap-detection,lora-simulation"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8847/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

# ============================================================================
# üåê NETWORKING & VOLUMES
# ============================================================================

networks:
  ultimate-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16

volumes:
  redis-data:
    driver: local
  qdrant-data:
    driver: local
  neo4j-data:
    driver: local
  neo4j-logs:
    driver: local
  ollama-data:
    driver: local
  conversation_transcripts:
    driver: local
  fact_checking_data:
    driver: local

# ============================================================================
# üèÜ ULTIMATE AI ORCHESTRATION ARCHITECTURE v10 + V5 + RESEARCH PAPER SUITE + V7 BASE LOGIC AGENT + QUANTUM A2A + LLM INTEGRATION + ENHANCED RESEARCH AGENT V3 + CONFIDENCE-DRIVEN LORA SYSTEM
# ============================================================================
# 
# TOTAL SERVICES: 60+ containers (INCLUDING V7 BASE LOGIC AGENT + RESEARCH PAPER GENERATION SUITE + COLLABORATIVE QUANTUM AGENT + LLM-INTEGRATED GAP DETECTION + ENHANCED RESEARCH AGENT V3 + CONFIDENCE-DRIVEN LORA SYSTEM)
# ARCHITECTURE LAYERS: 9 tiers + Enhanced Mathematical Verification + Research Paper Generation + V7 Logic Foundation + Quantum A2A Coordination + LLM Fast Chat Integration + Ultimate Research Intelligence + Confidence-Driven Learning
# 1. üß† HIGH-RANK ADAPTER - Ultimate Strategic Steering
# 2. üéØ META-ORCHESTRATION CONTROLLER - Strategic Logic  
# 3. ‚ö° ENHANCED EXECUTION SUITE - 8-Phase Orchestration
# 4. üöÄ LLM-INTEGRATED GAP DETECTION LAYER - Fast Chat with Background LoRA
# 5. üî¨ ENHANCED RESEARCH AGENT V3 - Ultimate Knowledge Integration with 15-minute LoRA
# 6. üßÆ V5 ENHANCED MATHEMATICAL ORCHESTRATOR - SymPy Verification
# 7. üìÑ RESEARCH PAPER GENERATION SUITE - Publication Excellence
# 8. ü§ñ V7 BASE LOGIC AGENT - Quantum-Ready Foundation + Einstein Puzzle Solver
# 9. üåå COLLABORATIVE QUANTUM AGENT - Quantum A2A Coordination
# 10. üß†üí° CONFIDENCE-DRIVEN LORA SYSTEM - Automatic Knowledge Gap Detection and Learning
# 11. üöÄ COMPREHENSIVE TEST SUITE - 100+ Questions Multi-LLM Testing
# 
# ORCHESTRATION PHASES: 8 phases + Mathematical Verification + Fact-Checking + Research Generation + AI Humanization + Logic Puzzle Solving + Quantum Processing + LLM Chat Integration + Ultimate Research Intelligence + Confidence-Driven Learning
# STRATEGIC STEERING: Ultimate meta-reasoning + Mathematical Precision + Accuracy Enhancement + Publication Excellence + Constraint Satisfaction + Quantum Advantage + Fast LLM Integration + Knowledge Integration Excellence + Automatic Gap Detection
# LLM INTEGRATION FEATURES: 4 Response Modes (Instant/Fast/Thorough/Research), Smart Gap Detection, Background LoRA Creation, Chat History, Multi-Field Expertise
# ENHANCED RESEARCH FEATURES: 15-minute LoRA timeout, RAG 2025 circular growth, Anti-hallucination engine, Parallel knowledge acquisition, Meta-learning capabilities
# CONFIDENCE-DRIVEN FEATURES: Real-time confidence monitoring, "I don't know" detection, Automatic LoRA triggering, Knowledge gap prioritization, Learning analytics
# MATHEMATICAL FEATURES: SymPy Integration, Real-time Correction, 100% Arithmetic Accuracy
# RESEARCH PAPER FEATURES: ChatGPT-style Interface, Infinite Elaboration, AI Detection Mitigation, Publication Quality 9.5/10
# V7 LOGIC FEATURES: 91.7% Classification Accuracy, 100% Einstein Puzzle Success, Multi-Attribute Constraint Matrix, AI Failsafe
# QUANTUM A2A FEATURES: 5-Phase Quantum Processing (Superposition, Entanglement, Interference, Observation, Collapse), Quantum Advantage Calculation
# TESTING FEATURES: 100+ Questions, Multi-LLM, Zebra Puzzles, Baby Math, Complex Math, Full JSON Output
# 
# üöÄ DEPLOYMENT COMMANDS:
# docker compose -f docker-compose-v10-ultimate.yml up -d
# docker compose -f docker-compose-v10-ultimate.yml --profile testing up  # Include testing services
# docker compose -f docker-compose-v10-ultimate.yml --profile v7-testing up  # Include V7 Logic Agent testing
# docker compose -f docker-compose-v10-ultimate.yml --profile confidence-testing up  # Include Confidence System testing
# docker compose -f docker-compose-v10-ultimate.yml --profile confidence-deployment up  # Include Confidence System deployment
# docker compose -f docker-compose-v10-ultimate.yml --profile comprehensive-testing run --rm comprehensive-test-suite
# docker compose -f docker-compose-v10-ultimate.yml logs -f confidence-driven-lora-creator
# docker compose -f docker-compose-v10-ultimate.yml logs -f ultimate-chat-orchestrator-with-confidence
# docker compose -f docker-compose-v10-ultimate.yml logs -f v7-base-logic-agent
# docker compose -f docker-compose-v10-ultimate.yml logs -f collaborative-quantum-agent
# docker compose -f docker-compose-v10-ultimate.yml logs -f llm-integrated-gap-detector
# docker compose -f docker-compose-v10-ultimate.yml logs -f enhanced-research-agent-v3
# docker compose -f docker-compose-v10-ultimate.yml exec confidence-driven-lora-creator curl http://localhost:8848/health
# docker compose -f docker-compose-v10-ultimate.yml exec ultimate-chat-orchestrator-with-confidence curl http://localhost:8950/health
# docker compose -f docker-compose-v10-ultimate.yml exec collaborative-quantum-agent curl http://localhost:8975/health
# docker compose -f docker-compose-v10-ultimate.yml exec llm-integrated-gap-detector curl http://localhost:8997/health
# docker compose -f docker-compose-v10-ultimate.yml exec enhanced-research-agent-v3 curl http://localhost:8999/health
# 
# üßÆ V5 MATHEMATICAL TESTING:
# docker compose -f docker-compose-v10-ultimate.yml run --rm sympy-verification-tester
# 
# ü§ñ V7 LOGIC AGENT TESTING:
# docker compose -f docker-compose-v10-ultimate.yml --profile v7-testing run --rm v7-logic-agent-tester
# 
# üß†üí° CONFIDENCE-DRIVEN SYSTEM TESTING:
# docker compose -f docker-compose-v10-ultimate.yml --profile confidence-testing run --rm confidence-driven-system-tester
# 
# üöÄ CONFIDENCE-DRIVEN SYSTEM DEPLOYMENT:
# docker compose -f docker-compose-v10-ultimate.yml --profile confidence-deployment run --rm confidence-driven-system-deployer
# 
# üåå QUANTUM A2A TESTING:
# curl -X POST http://localhost:8975/quantum/process -H "Content-Type: application/json" -d '{"query":"What is 2+2?","agent_responses":[{"agent_id":"math","answer":"4","confidence":0.9},{"agent_id":"logic","answer":"four","confidence":0.8}]}'
# 
# üöÄ LLM-INTEGRATED GAP DETECTION TESTING:
# curl -X POST http://localhost:8997/chat -H "Content-Type: application/json" -d '{"message":"What is quantum computing?","mode":"fast","field":"quantum_computing"}'
# curl -X POST http://localhost:8996/chat -H "Content-Type: application/json" -d '{"message":"Latest developments in AI","mode":"thorough","user_id":"test_user"}'
# 
# üß†üí° CONFIDENCE-DRIVEN SYSTEM TESTING:
# curl -X POST http://localhost:8848/assess_confidence -H "Content-Type: application/json" -d '{"query":"What is machine learning?","response":"I am not sure about the details","confidence_score":0.2,"response_time":1.5,"model_used":"test"}'
# curl -X POST http://localhost:8848/report_uncertainty -d 'query=What is quantum computing&response=I dont know much about that'
# curl -X POST http://localhost:8950/chat -H "Content-Type: application/json" -d '{"message":"Explain quantum superposition","confidence_reporting":true}'
# curl -X GET http://localhost:8848/knowledge_gaps
# curl -X GET http://localhost:8848/confidence_analytics
# curl -X GET http://localhost:8950/confidence_insights
# curl -X GET http://localhost:8847/demo  # Confidence Demo Service
# 
# üî¨ ENHANCED RESEARCH AGENT V3 TESTING:
# curl -X POST http://localhost:8999/research/generate -H "Content-Type: application/json" -d '{"topic":"Quantum Machine Learning for Drug Discovery","field":"quantum_computing","target_quality":9.2,"max_lora_wait_minutes":15}'
# curl -X POST http://localhost:8999/research/quick -H "Content-Type: application/json" -d '{"query":"Latest developments in AI research","max_wait_seconds":300}'
# curl -X GET http://localhost:8999/research/status
# curl -X GET http://localhost:8999/systems/utilized
# 
# üöÄ COMPREHENSIVE HYPER TESTING (100+ Questions, Multi-LLM):
# docker compose -f docker-compose-v10-ultimate.yml --profile comprehensive-testing run --rm comprehensive-test-suite
# 
# üìä MONITORING DASHBOARDS:
# Ultimate Architecture: http://localhost:9001
# V5 Mathematical Monitoring: http://localhost:3001
# Enhanced Fact-Checker: http://localhost:8885
# V5 Mathematical Orchestrator: http://localhost:8990
# V7 Base Logic Agent: http://localhost:8991
# Collaborative Quantum Agent: http://localhost:8975
# LLM-Integrated Gap Detector: http://localhost:8997
# LLM Chat Interface: http://localhost:8996
# Background LoRA Manager: http://localhost:8994
# Original LoRA Gap Generator: http://localhost:8993
# Enhanced Research Agent v3: http://localhost:8999
# Research Paper Frontend: http://localhost:3005
# AI Detection Mitigator: http://localhost:5001
# Enhanced Research Backend: http://localhost:5000
# Confidence-Driven LoRA Creator: http://localhost:8848
# Ultimate Chat Orchestrator with Confidence: http://localhost:8950
# Confidence Demo Service: http://localhost:8847
# 
# üß†üí° CONFIDENCE-DRIVEN LORA SYSTEM CAPABILITIES:
# ‚Ä¢ Real-time confidence monitoring and assessment
# ‚Ä¢ Explicit uncertainty detection ("I don't know" phrases)
# ‚Ä¢ Automatic LoRA creation when confidence drops below thresholds
# ‚Ä¢ Knowledge gap prioritization based on frequency and severity
# ‚Ä¢ Domain-specific gap classification (AI/ML, Quantum, Medicine, etc.)
# ‚Ä¢ Learning analytics and improvement tracking
# ‚Ä¢ Integration with existing LoRA pipeline and orchestration
# ‚Ä¢ Transparent confidence reporting to users
# ‚Ä¢ Conversation-aware gap detection and learning
# ‚Ä¢ Background monitoring and analysis
# 
# ü§ñüí° ULTIMATE CHAT ORCHESTRATOR ENHANCED FEATURES:
# ‚Ä¢ Multi-agent coordination with confidence assessment
# ‚Ä¢ Real-time knowledge gap detection during conversations
# ‚Ä¢ Automatic learning triggers when uncertainty detected
# ‚Ä¢ Transparent confidence reporting to users
# ‚Ä¢ Conversation history with confidence tracking
# ‚Ä¢ User recommendations based on confidence levels
# ‚Ä¢ Integration with all Ultimate AI Architecture layers
# ‚Ä¢ Self-improving conversation capabilities
# 
# üß™ CONFIDENCE SYSTEM TESTING CAPABILITIES:
# ‚Ä¢ Comprehensive service health monitoring
# ‚Ä¢ Confidence assessment functionality testing
# ‚Ä¢ Uncertainty detection with specialized queries
# ‚Ä¢ LoRA triggering validation
# ‚Ä¢ Knowledge gap tracking verification
# ‚Ä¢ End-to-end pipeline testing
# ‚Ä¢ Performance analytics validation
# ‚Ä¢ JSON output with detailed reporting
# 
# üöÄ CONFIDENCE SYSTEM DEPLOYMENT FEATURES:
# ‚Ä¢ Automated prerequisites checking
# ‚Ä¢ Docker image building and validation
# ‚Ä¢ Service deployment orchestration
# ‚Ä¢ Health monitoring and integration testing
# ‚Ä¢ Comprehensive deployment reporting
# ‚Ä¢ Error detection and recovery
# 
# üéØ CONFIDENCE DEMO SERVICE FEATURES:
# ‚Ä¢ Standalone demonstration without Docker dependencies
# ‚Ä¢ Real-time confidence assessment simulation
# ‚Ä¢ Knowledge gap detection and LoRA triggering demo
# ‚Ä¢ Priority-based learning queue demonstration
# ‚Ä¢ Detailed analytics and improvement tracking
# ‚Ä¢ REST API for integration testing
# 
# üöÄ LLM INTEGRATION WORKFLOW:
# ‚Ä¢ Phase 1: Ultra-fast Gap Analysis (< 1s) - Field detection, complexity analysis, confidence assessment
# ‚Ä¢ Phase 2: Smart Response Routing - Instant/Fast/Thorough/Research mode selection
# ‚Ä¢ Phase 3: Multi-Path Generation - Cache ‚Üí Ollama ‚Üí RAG ‚Üí LoRA routing
# ‚Ä¢ Phase 4: Background Improvement - Non-blocking LoRA creation for future enhancement
# ‚Ä¢ Phase 5: Chat Management - History maintenance, session tracking, confidence reporting
# 
# üî¨ ENHANCED RESEARCH WORKFLOW:
# ‚Ä¢ Phase 1: Comprehensive Knowledge Assessment - LLM gap analysis, RAG assessment, concept gap identification
# ‚Ä¢ Phase 2: Parallel Knowledge Acquisition - LoRA creation, RAG circular growth, concept crawling (up to 15 minutes)
# ‚Ä¢ Phase 3: Anti-Hallucination Content Generation - Verified content with hedging language and source verification
# ‚Ä¢ Phase 4: Quality Assurance Pipeline - Fact-checking, content quality assessment, final verification
# ‚Ä¢ Phase 5: Meta-Learning Integration - Pattern analysis and strategy optimization for future research
# 
# üß†üí° CONFIDENCE-DRIVEN LEARNING WORKFLOW:
# ‚Ä¢ Phase 1: Real-time Confidence Assessment - Monitor AI responses for uncertainty indicators
# ‚Ä¢ Phase 2: Knowledge Gap Detection - Classify gaps by domain, severity, and frequency
# ‚Ä¢ Phase 3: Priority-based Learning Queue - Prioritize gaps based on importance and impact
# ‚Ä¢ Phase 4: Targeted Content Gathering - Use enhanced crawler for domain-specific content
# ‚Ä¢ Phase 5: Specialized LoRA Creation - Create focused LoRAs to fill specific knowledge gaps
# ‚Ä¢ Phase 6: Improvement Validation - Track learning progress and confidence improvements
# ‚Ä¢ Phase 7: Continuous Optimization - Refine gap detection and learning strategies
# ============================================================================ 